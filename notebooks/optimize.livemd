# Optimization with Scholar.Optimize.GoldenSection

```elixir
Mix.install([
  {:scholar, path: "."},
  {:nx, "~> 0.9"},
  {:kino_vega_lite, "~> 0.1"}
])
```

## Introduction

`Scholar.Optimize.GoldenSection` provides the Golden Section search algorithm for scalar (univariate) function minimization.

## Scalar Optimization

Scalar optimization finds the minimum of a function of one variable within a specified bracket.

### Simple Parabola

Let's minimize $f(x) = (x - 3)^2$, which has its minimum at $x = 3$.

```elixir
alias Scholar.Optimize.GoldenSection

# Define the objective function
fun = fn x -> Nx.pow(Nx.subtract(x, 3), 2) end

# Find the minimum using Golden Section
result = GoldenSection.minimize(fun, bracket: {0.0, 5.0})

IO.puts("Minimum found at x = #{Nx.to_number(result.x)}")
IO.puts("Function value: #{Nx.to_number(result.fun)}")
IO.puts("Converged: #{Nx.to_number(result.converged) == 1}")
IO.puts("Iterations: #{Nx.to_number(result.iterations)}")
IO.puts("Function evaluations: #{Nx.to_number(result.fun_evals)}")
```

### Minimizing a Trigonometric Function

Find the minimum of $\sin(x)$ in the interval $[0, 2\pi]$:

```elixir
alias Scholar.Optimize.GoldenSection

fun = fn x -> Nx.sin(x) end

result = GoldenSection.minimize(fun, bracket: {0.0, 2 * :math.pi()})

expected_x = 3 * :math.pi() / 2  # 3π/2 ≈ 4.712

IO.puts("Minimum found at x = #{Nx.to_number(result.x)}")
IO.puts("Expected: #{expected_x}")
IO.puts("sin(x) at minimum: #{Nx.to_number(result.fun)}")
```

### Shifted Parabola

Minimize $f(x) = (x + 2)^2 + 1$, which has minimum at $x = -2$ with value $1$:

```elixir
alias Scholar.Optimize.GoldenSection

fun = fn x -> Nx.add(Nx.pow(Nx.add(x, 2), 2), 1) end

result = GoldenSection.minimize(fun, bracket: {-5.0, 5.0})

IO.puts("Minimum found at x = #{Nx.to_number(result.x)}")
IO.puts("Function value: #{Nx.to_number(result.fun)}")
IO.puts("Expected: x = -2, f(x) = 1")
```

## Golden Section Algorithm

The Golden Section search is a derivative-free optimization technique for finding the minimum of a unimodal function within a specified interval. It uses the golden ratio $\phi = \frac{\sqrt{5} - 1}{2} \approx 0.618$ to select interior points.

**Key properties:**
- Guaranteed convergence for unimodal functions
- Bracket width decreases by factor $\phi \approx 0.618$ per iteration
- Linear convergence rate
- Requires only function evaluations (no derivatives)

```elixir
alias Scholar.Optimize.GoldenSection

fun = fn x -> Nx.pow(Nx.subtract(x, 1), 2) end

result = GoldenSection.minimize(fun, bracket: {-5.0, 5.0}, tol: 1.0e-10, maxiter: 100)

IO.puts("Result from GoldenSection.minimize:")
IO.puts("  x = #{Nx.to_number(result.x)}")
IO.puts("  f(x) = #{Nx.to_number(result.fun)}")
IO.puts("  Iterations: #{Nx.to_number(result.iterations)}")
IO.puts("  Function evaluations: #{Nx.to_number(result.fun_evals)}")
```

## GPU/JIT Compatibility

Golden Section is implemented as a pure `defn` function, making it fully compatible with JIT compilation and GPU execution:

```elixir
alias Scholar.Optimize.GoldenSection

fun = fn x -> Nx.pow(Nx.subtract(x, 3), 2) end
opts = [bracket: {0.0, 5.0}, tol: 1.0e-8, maxiter: 500]

# This works with jit_apply for GPU acceleration
result = Nx.Defn.jit_apply(&GoldenSection.minimize/2, [fun, opts])

IO.puts("JIT-compiled result:")
IO.puts("  x = #{Nx.to_number(result.x)}")
IO.puts("  Converged: #{Nx.to_number(result.converged) == 1}")
```

## Options

### Tolerance

Control convergence tolerance (bracket width threshold):

```elixir
alias Scholar.Optimize.GoldenSection

fun = fn x -> Nx.pow(Nx.subtract(x, 2), 2) end

# Default tolerance (1.0e-8)
result_default = GoldenSection.minimize(fun, bracket: {0.0, 5.0})

# Looser tolerance (faster)
result_loose = GoldenSection.minimize(fun, bracket: {0.0, 5.0}, tol: 1.0e-4)

IO.puts("Default tolerance (1e-8): #{Nx.to_number(result_default.iterations)} iterations")
IO.puts("Loose tolerance (1e-4): #{Nx.to_number(result_loose.iterations)} iterations")
```

### Maximum Iterations

Limit the number of iterations:

```elixir
alias Scholar.Optimize.GoldenSection

fun = fn x -> Nx.pow(Nx.subtract(x, 2), 2) end

result = GoldenSection.minimize(fun, bracket: {0.0, 100.0}, maxiter: 10)

IO.puts("With maxiter: 10")
IO.puts("  Converged: #{Nx.to_number(result.converged) == 1}")
IO.puts("  Iterations used: #{Nx.to_number(result.iterations)}")
IO.puts("  Final bracket width affects convergence")
```

## Result Struct

`GoldenSection.minimize/2` returns a `Scholar.Optimize.GoldenSection` struct:

```elixir
alias Scholar.Optimize.GoldenSection

fun = fn x -> Nx.pow(Nx.subtract(x, 3), 2) end

result = GoldenSection.minimize(fun, bracket: {0.0, 5.0})

IO.puts("Result struct fields:")
IO.puts("  x:          #{Nx.to_number(result.x)} (solution)")
IO.puts("  fun:        #{Nx.to_number(result.fun)} (function value at solution)")
IO.puts("  converged:  #{Nx.to_number(result.converged)} (1 = yes, 0 = no)")
IO.puts("  iterations: #{Nx.to_number(result.iterations)}")
IO.puts("  fun_evals:  #{Nx.to_number(result.fun_evals)}")
```
