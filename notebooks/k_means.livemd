<!-- livebook:{"persist_outputs":true} -->

# K-means clustering

```elixir
Mix.install([
  {:scholar, "~> 0.1"},
  {:exla, "~> 0.5.2"},
  {:explorer, "~> 0.5.6"},
  {:stb_image, "~> 0.6"},
  {:scidata, "~> 0.1.9"},
  {:req, "~> 0.3.6"},
  {:kino, "~> 0.9.0"},
  {:kino_vega_lite, "~> 0.1.8"}
])
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Introduction

The main purpose of this livebook is to introduce the KMeans clustering algorithm. We will explore KMeans in three different use cases.

```elixir
alias VegaLite, as: Vl
alias Scholar.Cluster.KMeans
require Explorer.DataFrame, as: DF
Nx.global_default_backend(EXLA.Backend)
seed = 42
```

<!-- livebook:{"output":true} -->

```
42
```

<!-- livebook:{"branch_parent_index":0} -->

## Iris Dataset

The first example we will focus on is the **Iris Dataset**. It is one of the most renowned datasets. It consists of 150 records describing three iris species: *Iris Setosa*, *Iris Virginica*, and *Iris Versicolor*. Our task will be to predict the species of given flowers.

<!-- livebook:{"break_markdown":true} -->

Firstly, we load the data, then we split it into Training Data (x) and Target (y) and cast those into Nx tensors.

```elixir
df = Explorer.Datasets.iris()
x = df |> DF.discard(["species"]) |> Nx.stack(axis: 1)

y =
  df[["species"]]
  |> DF.dummies(["species"])
  |> Nx.stack(axis: 1)
  |> Nx.argmax(axis: 1)

{x, y}
```

<!-- livebook:{"output":true} -->

```

00:58:22.040 [info] TfrtCpuClient created.

```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   f64[150][4]
   EXLA.Backend<host:0, 0.3940695604.3471966220.252682>
   [
     [5.1, 3.5, 1.4, 0.2],
     [4.9, 3.0, 1.4, 0.2],
     [4.7, 3.2, 1.3, 0.2],
     [4.6, 3.1, 1.5, 0.2],
     [5.0, 3.6, 1.4, 0.2],
     [5.4, 3.9, 1.7, 0.4],
     [4.6, 3.4, 1.4, 0.3],
     [5.0, 3.4, 1.5, 0.2],
     [4.4, 2.9, 1.4, 0.2],
     [4.9, 3.1, 1.5, 0.1],
     [5.4, 3.7, 1.5, 0.2],
     [4.8, 3.4, 1.6, 0.2],
     [4.8, ...],
     ...
   ]
 >,
 #Nx.Tensor<
   s64[150]
   EXLA.Backend<host:0, 0.3940695604.3471966220.252696>
   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]
 >}
```

### Exploratory Data Analysis

<!-- livebook:{"break_markdown":true} -->

An important part of Data Science workflow is so-called **Exploratory Data Analysis**. EDA helps us understand the data in a better way and suggests some efficient strategies to solve problems. There is no one specific course of action which defines good EDA. It should contain tabular summaries and plots showing relations between features.

<!-- livebook:{"break_markdown":true} -->

We start our EDA by finding the mean values of each feature by species.

```elixir
grouped_data = DF.group_by(df, "species")

DF.summarise(
  grouped_data,
  petal_length: mean(petal_length),
  petal_width: mean(petal_width),
  sepal_width: mean(sepal_width),
  sepal_length: mean(sepal_length)
)
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[3 x 5]
  species string ["Iris-setosa", "Iris-versicolor", "Iris-virginica"]
  petal_length float [1.464, 4.26, 5.552]
  petal_width float [0.2439999999999999, 1.3259999999999998, 2.026]
  sepal_width float [3.4180000000000006, 2.7700000000000005, 2.9739999999999998]
  sepal_length float [5.005999999999999, 5.936, 6.587999999999998]
>
```

We see that `petal_length` and `petal_width` are the most distinguishing features. Let's explore them a little bit more.

```elixir
Vl.new(title: [text: "Histograms of petal_length column by species", offset: 25, anchor: :middle])
|> Vl.data_from_values(df)
|> Vl.facet(
  [field: "species"],
  Vl.new(width: 200, height: 200)
  |> Vl.mark(:bar)
  |> Vl.encode_field(:x, "petal_length", type: :quantitative, bin: true)
  |> Vl.encode(:y, aggregate: :count, scale: [domain: [0, 55]])
)
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

```elixir
Vl.new(
  width: 300,
  height: 300,
  title: [
    text: "Scatterplot of data samples pojected on plane petal_width x petal_length",
    offset: 25
  ]
)
|> Vl.data_from_values(df)
|> Vl.mark(:circle)
|> Vl.encode_field(:x, "petal_length", type: :quantitative)
|> Vl.encode_field(:y, "petal_width", type: :quantitative)
|> Vl.encode_field(:color, "species")
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

```elixir
Vl.new(
  title: [
    text: "Scatterplot of data samples pojected on plane petal_width x petal_length by species",
    offset: 25,
    anchor: :middle
  ]
)
|> Vl.data_from_values(df)
|> Vl.facet(
  [field: "species"],
  Vl.new(width: 200, height: 200)
  |> Vl.mark(:point)
  |> Vl.encode_field(:x, "petal_length", type: :quantitative)
  |> Vl.encode_field(:y, "petal_width", type: :quantitative)
)
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

Now we have a better understanding of the data. Iris species have different petal widths and petal lengths. Iris *Setosa* has the smallest petal, *Versicolor* is medium size, and *Virginica* has the largest petal. We can ascertain that our analysis is correct and plot the so-called **Elbow plot**. The Elbow plot is a plot which presents Inertia vs the number of clusters. If there is a characteristic elbow, then we have a strong suggestion that the number of clusters is correct. Let's train KMeans models for a different number of clusters from range 1 to 11.

```elixir
clusterings = 1..11

models =
  for num_clusters <- clusterings do
    KMeans.fit(x, num_clusters: num_clusters, seed: seed)
  end

inertias = for model <- models, do: Nx.to_number(model.inertia)
```

<!-- livebook:{"output":true} -->

```
[680.8244, 152.36870647733906, 78.94084142614602, 57.34540931571816, 46.55405384615385,
 38.94595367106237, 35.19772367071281, 30.289863862635872, 28.597346795761275, 27.04065487453723,
 25.920956415204678]
```

```elixir
Vl.new(width: 600, height: 300, title: "Elbow Plot")
|> Vl.data_from_values(num_clusters: clusterings, inertia: inertias)
|> Vl.mark(:line)
|> Vl.encode_field(:x, "num_clusters",
  title: "Number of Clusters",
  type: :ordinal,
  axis: [label_angle: 0]
)
|> Vl.encode_field(:y, "inertia", title: "Inertia", type: :quantitative)
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

As you can see, we have the elbow when the number of clusters equals three. So this value of the parameter seems to be the best.

<!-- livebook:{"break_markdown":true} -->

In order to compare our clustering with the target labels, we need to ensure our clusters are in a matching order.

```elixir
defmodule Iris.Clusters do
  import Nx.Defn

  defn sort_clusters(model) do
    # We sort clusters by the first coordinate
    order = Nx.argsort(model.clusters[[.., 0]])
    labels_maping = Nx.argsort(order)

    %{
      model
      | labels: Nx.take(labels_maping, model.labels),
        clusters: Nx.take(model.clusters, order)
    }
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Iris.Clusters, <<70, 79, 82, 49, 0, 0, 10, ...>>, true}
```

```elixir
best_model = Enum.at(models, 2)
best_model = Iris.Clusters.sort_clusters(best_model)
accuracy = Scholar.Metrics.accuracy(best_model.labels, y)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32
  EXLA.Backend<host:0, 0.3940695604.3472228364.31001>
  0.8933333158493042
>
```

Accuracy is nearly 90% - that's pretty decent! Let's look at our results plotted on one of the previous plots.

```elixir
coords = [
  cluster_petal_length: best_model.clusters[[.., 2]] |> Nx.to_flat_list(),
  cluster_petal_width: best_model.clusters[[.., 3]] |> Nx.to_flat_list()
]

Vl.new(
  width: 300,
  height: 300,
  title: [
    text:
      "Scatterplot of data samples pojected on plane petal_width x petal_length with calculated centroids",
    offset: 25
  ]
)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values(df)
  |> Vl.mark(:circle)
  |> Vl.encode_field(:x, "petal_length", type: :quantitative)
  |> Vl.encode_field(:y, "petal_width", type: :quantitative)
  |> Vl.encode_field(:color, "species"),
  Vl.new()
  |> Vl.data_from_values(coords)
  |> Vl.mark(:circle, color: :green, size: 100)
  |> Vl.encode_field(:x, "cluster_petal_length", type: :quantitative)
  |> Vl.encode_field(:y, "cluster_petal_width", type: :quantitative)
])
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

As we expect ðŸ˜Ž

<!-- livebook:{"branch_parent_index":0} -->

## Clustering of pixel colors

The other interesting use case of KMeans clustering is pixel clustering. This technique replaces all pixels with similar colors (similar in terms of euclidean distance between RGB) with a centroid related to them.

<!-- livebook:{"break_markdown":true} -->

Let us start with loading the referral image.

```elixir
url =
  "https://pix4free.org/assets/library/2021-01-12/originals/san_francisco_california_golden_gate_bridge_water.jpg"

%{body: raw_image} = Req.get!(url)
image = StbImage.read_binary!(raw_image)

{height, width, _num_channels} = image.shape
image = StbImage.resize(image, div(height, 3), div(width, 3))
shape = image.shape

image_kino = image |> StbImage.to_binary(:jpg) |> Kino.Image.new(:jpeg)
```

Now we will try to use only ten colors to represent the same picture.

```elixir
x = image |> StbImage.to_nx() |> Nx.reshape({:auto, 3})

model =
  KMeans.fit(x,
    num_clusters: 10,
    num_runs: 10,
    max_iterations: 200,
    seed: seed
  )

repainted_x = Nx.take(model.clusters, model.labels)

tensor_to_image = fn x ->
  x
  |> Nx.reshape(shape)
  |> Nx.round()
  |> Nx.as_type({:u, 8})
  |> StbImage.from_nx()
  |> StbImage.to_binary(:jpg)
  |> Kino.Image.new(:jpeg)
end

repainted_x = tensor_to_image.(repainted_x)
```

Look that even though we use only ten colors, we can say without any doubt that this is the same image. Let's experiment more deeply. Now we will try 5, 10, 15, 20 and 40 colors and then compare the processed images with the original one.

```elixir
clusterings = [5, 10, 15, 20, 40]

models =
  for num_clusters <- clusterings do
    KMeans.fit(x, num_clusters: num_clusters, seed: seed)
  end
```

```elixir
image_boxes =
  for {model, num_clusters} <- Enum.zip(models, clusterings) do
    repainted_x = Nx.take(model.clusters, model.labels)

    image_kino = tensor_to_image.(repainted_x)

    Kino.Layout.grid(
      [Kino.Markdown.new("### Number of colors: #{num_clusters}"), image_kino],
      boxed: true
    )
  end

image_box =
  Kino.Layout.grid(
    [Kino.Markdown.new("### Original image"), image_kino],
    boxed: true
  )

Kino.Layout.grid(image_boxes ++ [image_box], columns: 2)
```

Look that even with only five colors can recognize the Golden Gate Bridge in the image. On the other hand, with only 40 colors we keep almost all details except the sky and water surface. Sky and water do not map well because there is a small gradient in changing colors. Pixel clustering is a great way to compress images drastically with small integration in their appearance.

<!-- livebook:{"branch_parent_index":0} -->

## Clustering images from Fashion-MNIST

The last example is the clustering problem on the Fashion-MNIST Dataset. The dataset consists of 60000 images 28 by 28 pixels of ten different parts of clothing. Let's dive into this clustering problem.

<!-- livebook:{"break_markdown":true} -->

Before we start, we define the StratifiedSplit module. The module trims input data and splits it, so the number of samples per class is the same for each.

```elixir
defmodule StratifiedSplit do
  import Nx.Defn

  defn trim_samples(x, labels, opts \\ []) do
    opts = keyword!(opts, [:num_classes, :samples_per_class])

    num_classes = opts[:num_classes]
    samples_per_class = opts[:samples_per_class]

    membership_mask = Nx.iota({1, num_classes}) == Nx.reshape(labels, {:auto, 1})

    indices =
      membership_mask
      |> Nx.argsort(axis: 0, direction: :desc)
      |> Nx.slice_along_axis(0, samples_per_class, axis: 0)
      |> Nx.flatten()

    {Nx.take(x, indices), Nx.take(labels, indices)}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, StratifiedSplit, <<70, 79, 82, 49, 0, 0, 13, ...>>, true}
```

Firstly, load the data and cast it into Nx tensors.

```elixir
{image_data, labels_data} = Scidata.FashionMNIST.download()

{images_binary, images_type, images_shape} = image_data
{num_samples, _num_channels = 1, image_height, image_width} = images_shape

images =
  images_binary
  |> Nx.from_binary(images_type)
  |> Nx.reshape({num_samples, :auto})
  |> Nx.divide(255)

{labels_binary, labels_type, _shape} = labels_data
target = Nx.from_binary(labels_binary, labels_type)

num_classes = 10
samples_per_class = 20

{images, target} =
  StratifiedSplit.trim_samples(images, target,
    num_classes: num_classes,
    samples_per_class: samples_per_class
  )

num_images = num_classes * samples_per_class
```

<!-- livebook:{"output":true} -->

```
200
```

Let's also define a function that will visualize an image in the tensor format for us.

```elixir
tensor_to_kino = fn x ->
  x
  |> Nx.reshape({image_height, image_width, 1})
  # Replicate the value into 3 channels for PNG
  |> Nx.broadcast({image_height, image_width, 3})
  |> Nx.multiply(255)
  |> Nx.as_type({:u, 8})
  |> StbImage.from_nx()
  |> StbImage.resize(112, 112)
  |> StbImage.to_binary(:png)
  |> Kino.Image.new(:png)
end
```

<!-- livebook:{"output":true} -->

```
#Function<42.3316493/1 in :erl_eval.expr/6>
```

Here is one of the images.

```elixir
tensor_to_kino.(images[0])
```

We will try some different numbers of clusters and then measure the quality of clustering.

```elixir
nums_clusters = 2..20

models =
  for num_clusters <- 2..20 do
    KMeans.fit(images, num_clusters: num_clusters, seed: seed)
  end
```

<!-- livebook:{"output":true} -->

```
[
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.40205>
      3
    >,
    clusters: #Nx.Tensor<
      f32[2][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.40252>
      [
        [0.0, 3.501400715322234e-5, 1.4005602861288935e-4, 3.851540677715093e-4, 3.501400933600962e-4, 7.002801285125315e-4, 4.901961074210703e-4, 0.009278712794184685, 0.04635854437947273, 0.09737396240234375, 0.22310924530029297, 0.29975488781929016, 0.32121849060058594, 0.3055672347545624, 0.3146008551120758, 0.36162465810775757, 0.32324934005737305, 0.3010154068470001, 0.20304621756076813, 0.06995797902345657, 0.019502801820635796, 0.003641456598415971, 0.003641456598415971, 0.0034313725773245096, 0.0025560224894434214, 0.0010504202218726277, 3.501400715322234e-5, 0.0, 0.0, 0.0, 3.501400715322234e-5, 4.901961074210703e-4, 5.252101109363139e-4, 0.006232493091374636, 0.05105042830109596, 0.13872550427913666, 0.2501050531864166, 0.37622547149658203, 0.533753514289856, 0.6370097994804382, 0.7304272055625916, 0.7347339391708374, 0.7232843637466431, 0.7482843399047852, 0.7121148109436035, 0.6053571105003357, 0.5261555314064026, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.40254>
      10950.6201171875
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.40258>
      [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.42080>
      3
    >,
    clusters: #Nx.Tensor<
      f32[3][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.42126>
      [
        [0.0, 5.9417710872367024e-5, 1.1883542174473405e-4, 2.376708434894681e-4, 2.376708434894681e-4, 4.753416869789362e-4, 2.376708434894681e-4, 0.013071895577013493, 0.05971479415893555, 0.12269756942987442, 0.27730244398117065, 0.3171122968196869, 0.2941770851612091, 0.2795603275299072, 0.28009507060050964, 0.31200236082077026, 0.2995246648788452, 0.3170528709888458, 0.25864526629447937, 0.07664884626865387, 0.02192513458430767, 1.7825313261710107e-4, 4.1592397610656917e-4, 3.565062361303717e-4, 1.1883542174473405e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.376708434894681e-4, 5.9417710872367024e-5, 0.009566251188516617, 0.05246583744883537, 0.11509210616350174, 0.23559121787548065, 0.3770647943019867, 0.5828877091407776, 0.6433154940605164, 0.7103387117385864, 0.7102198004722595, 0.6955437064170837, 0.7320261001586914, 0.675638735294342, 0.6002377271652222, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.42128>
      9246.3125
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.42130>
      [0, 0, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 0, 1, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 2, 1, 1, 1, 0, 0, 2, 0, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.44583>
      4
    >,
    clusters: #Nx.Tensor<
      f32[4][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.44629>
      [
        [0.0, 0.0, 1.8239855126012117e-4, 6.383949075825512e-4, 5.471956683322787e-4, 0.0010943913366645575, 7.295942050404847e-4, 0.004012768156826496, 0.02891017124056816, 0.06675787270069122, 0.19589604437351227, 0.31500229239463806, 0.3976288139820099, 0.38148659467697144, 0.3805745542049408, 0.4316461682319641, 0.3498404026031494, 0.2810761630535126, 0.13807569444179535, 0.05654355138540268, 0.008755129761993885, 5.471956101246178e-4, 8.20793560706079e-4, 0.0014591884100809693, 6.383949657902122e-4, 0.0, 9.119927563006058e-5, 0.0, 0.0, 0.0, 9.119927563006058e-5, 9.119927417486906e-4, 0.0012767899315804243, 0.0015503877075389028, 0.05243958532810211, 0.18467853963375092, 0.29092568159103394, 0.4202462434768677, 0.5493844747543335, 0.6998631954193115, 0.8229821920394897, 0.810123085975647, 0.7939808964729309, 0.8111263513565063, 0.7670770883560181, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.44631>
      8456.97265625
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.44633>
      [3, 3, 0, 3, 1, 1, 0, 1, 2, 2, 1, 3, 0, 3, 3, 1, 0, 1, 1, 2, 3, 3, 0, 3, 0, 1, 1, 1, 3, 2, 3, 3, 1, 3, 0, 1, 0, 1, 2, 2, 3, 3, 0, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.47717>
      5
    >,
    clusters: #Nx.Tensor<
      f32[5][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.47763>
      [
        [0.0, 1.7825313261710107e-4, 3.5650626523420215e-4, 7.130125304684043e-4, 7.130125304684043e-4, 3.5650626523420215e-4, 3.5650626523420215e-4, 0.038324419409036636, 0.14456328749656677, 0.19857396185398102, 0.2903743088245392, 0.3590017557144165, 0.29001784324645996, 0.21212123334407806, 0.22584672272205353, 0.28324422240257263, 0.3552584946155548, 0.40320855379104614, 0.3058823347091675, 0.17005346715450287, 0.052941180765628815, 1.7825313261710107e-4, 0.001247771899215877, 8.912656921893358e-4, 5.347594269551337e-4, 0.0, 1.7825313261710107e-4, 0.0, 0.0, 0.0, 0.0, 0.0010695188539102674, 1.7825313261710107e-4, 0.029411764815449715, 0.17843139171600342, 0.3386809527873993, 0.51871657371521, 0.6306595206260681, 0.7001782655715942, 0.7304812073707581, 0.7541888952255249, 0.7663101553916931, 0.7131907343864441, 0.7664884328842163, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.47765>
      7930.39501953125
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.47767>
      [0, 2, 3, 2, 1, 1, 3, 1, 4, 4, 2, 2, 3, 2, 2, 1, 3, 1, 1, 4, 2, 2, 3, 2, 3, 1, 1, 1, 2, 4, 0, 2, 1, 2, 3, 1, 3, 1, 4, 4, 0, 2, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.51453>
      5
    >,
    clusters: #Nx.Tensor<
      f32[6][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.51499>
      [
        [0.0, 1.9607844296842813e-4, 3.9215688593685627e-4, 7.843137718737125e-4, 5.882353289052844e-4, 3.9215688593685627e-4, 3.9215688593685627e-4, 0.04215686395764351, 0.1590196192264557, 0.20411762595176697, 0.3068627417087555, 0.3650980591773987, 0.2813725769519806, 0.20686273276805878, 0.2221568524837494, 0.2792156934738159, 0.34980395436286926, 0.4145098328590393, 0.3145098090171814, 0.18705882132053375, 0.058235298842191696, 1.9607844296842813e-4, 0.001372549100778997, 9.803923312574625e-4, 5.882353289052844e-4, 0.0, 1.9607844296842813e-4, 0.0, 0.0, 0.0, 0.0, 0.0011764706578105688, 1.9607844296842813e-4, 0.03235294297337532, 0.196274533867836, 0.3598039746284485, 0.5158823728561401, 0.615490198135376, 0.6762745380401611, 0.7176470160484314, 0.7488234639167786, 0.7762745022773743, 0.7133333086967468, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.51501>
      7494.529296875
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.51503>
      [0, 4, 2, 1, 1, 1, 2, 5, 3, 3, 1, 4, 2, 4, 4, 1, 2, 5, 1, 2, 4, 4, 2, 4, 2, 5, 1, 5, 1, 3, 0, 4, 1, 1, 2, 5, 2, 5, 3, 3, 0, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.55791>
      5
    >,
    clusters: #Nx.Tensor<
      f32[7][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.55837>
      [
        [0.0, 1.8674138118512928e-4, 3.7348276237025857e-4, 7.469655247405171e-4, 7.469655247405171e-4, 3.7348276237025857e-4, 5.602241726592183e-4, 0.040149394422769547, 0.15144725143909454, 0.19439774751663208, 0.30999064445495605, 0.37030816078186035, 0.2868347764015198, 0.21699346601963043, 0.23155930638313293, 0.2859010100364685, 0.3518207371234894, 0.41942110657691956, 0.3163398504257202, 0.17815124988555908, 0.05546218901872635, 1.8674138118512928e-4, 0.0013071896973997355, 0.0011204483453184366, 5.602241726592183e-4, 0.0, 1.8674138118512928e-4, 0.0, 0.0, 0.0, 0.0, 0.0011204483453184366, 3.7348276237025857e-4, 0.031185809522867203, 0.18692812323570251, 0.35536885261535645, 0.5211951732635498, 0.6207282543182373, 0.6804856061935425, 0.7208216786384583, 0.7510737180709839, 0.777404248714447, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.55839>
      7131.3193359375
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.55841>
      [0, 4, 2, 1, 1, 1, 2, 5, 3, 6, 1, 4, 2, 4, 4, 6, 2, 5, 1, 6, 4, 4, 2, 4, 2, 5, 1, 5, 1, 6, 0, 4, 1, 1, 2, 5, 2, 5, 3, 6, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.60760>
      6
    >,
    clusters: #Nx.Tensor<
      f32[8][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.60806>
      [
        [0.0, 2.063983556581661e-4, 4.127967113163322e-4, 8.255934226326644e-4, 6.19195110630244e-4, 4.127967113163322e-4, 4.127967113163322e-4, 0.029308566823601723, 0.13787409663200378, 0.18410733342170715, 0.28668728470802307, 0.3564499318599701, 0.28276577591896057, 0.21775025129318237, 0.23384933173656464, 0.29391124844551086, 0.36181631684303284, 0.40619194507598877, 0.2887512743473053, 0.17729617655277252, 0.05180598422884941, 2.063983556581661e-4, 0.0014447885332629085, 8.255934226326644e-4, 6.19195110630244e-4, 0.0, 2.063983556581661e-4, 0.0, 0.0, 0.0, 0.0, 0.0010319917928427458, 2.063983556581661e-4, 0.024561405181884766, 0.17667698860168457, 0.34571728110313416, 0.5162023305892944, 0.6243550181388855, 0.6893705129623413, 0.7110422849655151, 0.7525283694267273, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.60808>
      6816.73828125
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.60810>
      [0, 4, 2, 1, 1, 1, 2, 5, 3, 6, 1, 4, 2, 4, 1, 6, 2, 5, 1, 7, 4, 4, 2, 4, 2, 5, 1, 5, 1, 6, 0, 4, 1, 1, 1, 5, 2, 5, 3, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.66273>
      4
    >,
    clusters: #Nx.Tensor<
      f32[9][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.66319>
      [
        [0.0, 0.0, 0.0, 0.0, 0.0, 1.508295681560412e-4, 0.0, 1.508295681560412e-4, 0.04464555159211159, 0.1469080001115799, 0.2966817617416382, 0.35972851514816284, 0.33122172951698303, 0.31794869899749756, 0.3088989555835724, 0.35806941986083984, 0.3502262234687805, 0.3734539747238159, 0.364705890417099, 0.0865761786699295, 0.011161387898027897, 1.508295681560412e-4, 1.508295681560412e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.508295681560412e-4, 0.0, 0.02624434418976307, 0.19969835877418518, 0.43861234188079834, 0.7618401050567627, 0.776168942451477, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.66321>
      6590.8154296875
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.66323>
      [7, 0, 2, 3, 5, 5, 2, 1, 8, 8, 3, 0, 2, 0, 3, 5, 6, 1, 5, 4, 0, 0, 2, 0, 2, 1, 5, 1, 3, 8, 7, 3, 5, 3, 6, 1, 2, 1, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.72446>
      6
    >,
    clusters: #Nx.Tensor<
      f32[10][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.72492>
      [
        [0.0, 0.0, 0.0, 0.0, 0.0, 1.5686274855397642e-4, 0.0, 1.5686274855397642e-4, 0.04643137380480766, 0.1527843177318573, 0.3036862909793854, 0.34792155027389526, 0.32549017667770386, 0.3087058961391449, 0.30760782957077026, 0.35372552275657654, 0.34384310245513916, 0.3678431212902069, 0.3535686433315277, 0.0900392234325409, 0.011607843451201916, 1.5686274855397642e-4, 1.5686274855397642e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5686274855397642e-4, 0.0, 0.027294117957353592, 0.20768629014492035, 0.456156849861145, 0.7719215154647827, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.72494>
      6452.82421875
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.72496>
      [7, 0, 2, 3, 5, 5, 2, 1, 8, 8, 9, 0, 2, 0, 3, 5, 6, 1, 5, 4, 0, 0, 2, 0, 2, 1, 5, 1, 9, 8, 7, 9, 5, 9, 6, 1, 2, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.79192>
      5
    >,
    clusters: #Nx.Tensor<
      f32[11][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.79238>
      [
        [0.0, 3.016591363120824e-4, 3.016591363120824e-4, 0.0012066365452483296, 9.049774380400777e-4, 6.033182726241648e-4, 6.033182726241648e-4, 0.012368024326860905, 0.1381598860025406, 0.2422322928905487, 0.40452486276626587, 0.4615384638309479, 0.30648568272590637, 0.2190045416355133, 0.23438915610313416, 0.29713425040245056, 0.45610859990119934, 0.5276018381118774, 0.32428357005119324, 0.1381598860025406, 0.036500755697488785, 3.016591363120824e-4, 0.0015082956524565816, 0.0012066365452483296, 3.016591363120824e-4, 0.0, 3.016591363120824e-4, 0.0, 0.0, 0.0, 0.0, 0.0015082956524565816, 3.016591363120824e-4, 0.017496230080723763, 0.22292611002922058, 0.4105581045150757, 0.580995500087738, 0.7309200763702393, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.79240>
      6222.57080078125
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.79242>
      [0, 6, 1, 8, 8, 5, 1, 2, 10, 9, 8, 6, 1, 6, 7, 5, 1, 2, 7, 9, 6, 4, 1, 6, 1, 2, 5, 2, 7, 10, 0, 4, 8, 8, 8, 5, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.86570>
      6
    >,
    clusters: #Nx.Tensor<
      f32[12][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.86616>
      [
        [0.0, 0.0, 0.0, 0.0, 0.0, 2.4509805371053517e-4, 0.0, 2.4509805371053517e-4, 0.07230392098426819, 0.13308824598789215, 0.16789215803146362, 0.16225489974021912, 0.18480391800403595, 0.1924019604921341, 0.18406862020492554, 0.22230391204357147, 0.2132353037595749, 0.19191177189350128, 0.20441177487373352, 0.0784313827753067, 0.018137255683541298, 2.4509805371053517e-4, 7.35294190235436e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4509805371053517e-4, 7.35294132027775e-4, 0.04264706000685692, 0.3142157196998596, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.86618>
      6077.1826171875
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.86620>
      [7, 3, 2, 3, 5, 5, 6, 1, 8, 4, 5, 0, 2, 0, 3, 10, 6, 1, 5, 4, 0, 11, 2, 0, 2, 1, 5, 1, 9, 10, 7, 11, 3, 9, 3, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.94521>
      5
    >,
    clusters: #Nx.Tensor<
      f32[13][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.94567>
      [
        [0.0, 3.267973952461034e-4, 3.267973952461034e-4, 0.0013071895809844136, 9.803922148421407e-4, 6.535947904922068e-4, 6.535947904922068e-4, 0.003594771260395646, 0.12156862765550613, 0.26241829991340637, 0.4372549057006836, 0.5, 0.3320261538028717, 0.23725491762161255, 0.25392159819602966, 0.32189545035362244, 0.4941176474094391, 0.5633987188339233, 0.3513071835041046, 0.13496731221675873, 0.03790849819779396, 3.267973952461034e-4, 0.0016339869471266866, 6.535947904922068e-4, 3.267973952461034e-4, 0.0, 3.267973952461034e-4, 0.0, 0.0, 0.0, 0.0, 9.803922148421407e-4, 3.267973952461034e-4, 0.017320262268185616, 0.1954248547554016, 0.3732026517391205, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.94569>
      5979.98828125
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.94571>
      [0, 6, 5, 11, 11, 12, 3, 1, 4, 4, 11, 6, 5, 10, 11, 12, 3, 1, 11, 7, 10, 6, 5, 10, 3, 1, 11, 1, 11, 4, 0, 6, 11, 11, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.103074>
      5
    >,
    clusters: #Nx.Tensor<
      f32[14][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.103120>
      [
        [0.0, 0.0, 0.0, 0.0, 0.0, 2.3068052541930228e-4, 0.0, 0.0, 0.06828143447637558, 0.22168396413326263, 0.31026530265808105, 0.30980393290519714, 0.30011534690856934, 0.27843138575553894, 0.28765860199928284, 0.3312572240829468, 0.3100346028804779, 0.3550172746181488, 0.3289504051208496, 0.11764705926179886, 0.017070358619093895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3068052541930228e-4, 0.0, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.103122>
      5835.94775390625
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.103124>
      [7, 0, 2, 3, 5, 5, 6, 1, 8, 4, 9, 0, 2, 0, 3, 13, 6, 1, 5, 4, 0, 11, 2, 12, 2, 1, 5, 1, 9, 10, 7, 11, 3, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.112200>
      4
    >,
    clusters: #Nx.Tensor<
      f32[15][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.112246>
      [
        [0.0, 6.535947904922068e-4, 6.535947904922068e-4, 0.0026143791619688272, 0.0013071895809844136, 0.0013071895809844136, 0.0013071895809844136, 0.0, 0.0, 0.0797385647892952, 0.41111111640930176, 0.6673202514648438, 0.4562091529369354, 0.3379085063934326, 0.35424837470054626, 0.40784311294555664, 0.5908496975898743, 0.6248366236686707, 0.2980392277240753, 0.024183006957173347, 0.0, 0.0, 0.0019607844296842813, 6.535947904922068e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013071895809844136, 6.535947904922068e-4, 0.0, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.112248>
      5697.43701171875
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.112250>
      [0, 12, 1, 8, 8, 5, 1, 2, 10, 14, 8, 6, 1, 6, 7, 5, 13, 2, 7, 9, 6, 12, 1, 3, 1, 2, 5, 2, 7, 14, 11, 12, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.121957>
      5
    >,
    clusters: #Nx.Tensor<
      f32[16][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.122003>
      [
        [0.0, 5.602241144515574e-4, 5.602241144515574e-4, 0.0022408964578062296, 0.0011204482289031148, 0.0011204482289031148, 0.0011204482289031148, 0.0, 0.0, 0.06834734231233597, 0.3523809611797333, 0.5826330780982971, 0.3932773172855377, 0.28963586688041687, 0.3036414682865143, 0.3495798110961914, 0.5064426064491272, 0.5355742573738098, 0.2554621994495392, 0.02072829194366932, 0.0, 5.602241144515574e-4, 0.002801120514050126, 5.602241144515574e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0011204482289031148, 5.602241144515574e-4, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.122005>
      5572.9951171875
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.122007>
      [0, 12, 1, 8, 8, 5, 1, 2, 10, 14, 8, 4, 1, 6, 7, 9, 13, 2, 7, 15, 6, 12, 1, 6, 13, 2, 5, 2, 7, 14, 11, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.132316>
      5
    >,
    clusters: #Nx.Tensor<
      f32[17][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.132362>
      [
        [0.0, 3.267973952461034e-4, 3.267973952461034e-4, 0.0013071895809844136, 9.803922148421407e-4, 6.535947904922068e-4, 6.535947904922068e-4, 0.003594771260395646, 0.12156862765550613, 0.26241829991340637, 0.4372549057006836, 0.5, 0.3320261538028717, 0.23725491762161255, 0.25392159819602966, 0.32189545035362244, 0.4941176474094391, 0.5633987188339233, 0.3513071835041046, 0.13496731221675873, 0.03790849819779396, 3.267973952461034e-4, 0.0016339869471266866, 6.535947904922068e-4, 3.267973952461034e-4, 0.0, 3.267973952461034e-4, 0.0, 0.0, 0.0, 0.0, 9.803922148421407e-4, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.132364>
      5474.91015625
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.132366>
      [0, 15, 3, 11, 11, 16, 3, 1, 8, 4, 11, 15, 5, 10, 14, 12, 3, 1, 14, 7, 10, 15, 5, 10, 13, 16, 16, 1, 14, 4, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.143277>
      5
    >,
    clusters: #Nx.Tensor<
      f32[18][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.143323>
      [
        [0.0, 3.5650626523420215e-4, 3.5650626523420215e-4, 0.0014260250609368086, 0.0010695188539102674, 7.130125304684043e-4, 7.130125304684043e-4, 0.003921568859368563, 0.10837790369987488, 0.23814617097377777, 0.448128342628479, 0.5351158380508423, 0.36221033334732056, 0.25704100728034973, 0.2770053744316101, 0.3383244276046753, 0.5115864276885986, 0.5593582987785339, 0.32477715611457825, 0.11800356209278107, 0.039928700774908066, 3.5650626523420215e-4, 0.0017825312679633498, 7.130125304684043e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.143325>
      5393.45166015625
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.143327>
      [0, 15, 17, 11, 11, 16, 3, 1, 8, 4, 11, 15, 5, 10, 14, 12, 3, 1, 14, 7, 10, 15, 17, 10, 13, 16, 16, 1, 14, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.154840>
      5
    >,
    clusters: #Nx.Tensor<
      f32[19][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.154886>
      [
        [0.0, 3.5650626523420215e-4, 3.5650626523420215e-4, 0.0014260250609368086, 0.0010695188539102674, 7.130125304684043e-4, 7.130125304684043e-4, 0.003921568859368563, 0.10837790369987488, 0.23814617097377777, 0.448128342628479, 0.5351158380508423, 0.36221033334732056, 0.25704100728034973, 0.2770053744316101, 0.3383244276046753, 0.5115864276885986, 0.5593582987785339, 0.32477715611457825, 0.11800356209278107, 0.039928700774908066, 3.5650626523420215e-4, 0.0017825312679633498, 7.130125304684043e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.154888>
      5257.630859375
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.154890>
      [0, 15, 17, 11, 11, 16, 3, 1, 8, 4, 11, 15, 5, 10, 14, 12, 3, 1, 14, 7, 10, 15, 17, 10, 13, 18, 16, 1, ...]
    >
  },
  %Scholar.Cluster.KMeans{
    num_iterations: #Nx.Tensor<
      s64
      EXLA.Backend<host:0, 0.3940695604.3472228364.167005>
      5
    >,
    clusters: #Nx.Tensor<
      f32[20][784]
      EXLA.Backend<host:0, 0.3940695604.3472228364.167051>
      [
        [0.0, 5.602241144515574e-4, 5.602241144515574e-4, 0.0022408964578062296, 0.0011204482289031148, 0.0011204482289031148, 0.0011204482289031148, 0.0, 0.0, 0.06834734231233597, 0.3523809611797333, 0.5826330780982971, 0.3932773172855377, 0.28963586688041687, 0.3036414682865143, 0.3495798110961914, 0.5064426064491272, 0.5355742573738098, 0.2554621994495392, 0.02072829194366932, 0.0, 5.602241144515574e-4, 0.002801120514050126, 5.602241144515574e-4, 0.0, 0.0, 0.0, 0.0, 0.0, ...],
        ...
      ]
    >,
    inertia: #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3940695604.3472228364.167053>
      5166.4755859375
    >,
    labels: #Nx.Tensor<
      s64[200]
      EXLA.Backend<host:0, 0.3940695604.3472228364.167055>
      [0, 15, 17, 11, 11, 16, 3, 1, 8, 4, 11, 15, 5, 10, 14, 12, 3, 1, 14, 7, 10, 15, 17, 10, 13, 18, 16, ...]
    >
  }
]
```

```elixir
data = [
  num_clusters: nums_clusters,
  inertia: for(model <- models, do: Nx.to_number(model.inertia))
]

Vl.new(width: 600, height: 300, title: "Elbow Plot")
|> Vl.data_from_values(data)
|> Vl.mark(:line)
|> Vl.encode_field(:x, "num_clusters",
  title: "Number of Clusters",
  type: :ordinal,
  axis: [label_angle: 0]
)
|> Vl.encode_field(:y, "inertia",
  title: "Inertia",
  type: :quantitative,
  scale: [domain: [4800, 11500]]
)
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

Look that this time there is no elbow on a plot. We need to use a different method to predict the number of classes. Now we will use *Silhouette Score*. It is a metric that indicates the quality of clustering. The higher score we achieve, the better clustering we get. However, we should be aware that Silhouette Score is just a heuristic and not always works.

```elixir
silhouette_scores =
  for {model, num_clusters} <- Enum.zip(models, nums_clusters) do
    Scholar.Metrics.Clustering.silhouette_score(images, model.labels, num_clusters: num_clusters)
    |> Nx.to_number()
  end
```

<!-- livebook:{"output":true} -->

```
[0.1867797076702118, 0.19426067173480988, 0.18552090227603912, 0.16728746891021729,
 0.14723309874534607, 0.1479528844356537, 0.1612304151058197, 0.12663854658603668,
 0.11995869874954224, 0.13496387004852295, 0.12647153437137604, 0.13648101687431335,
 0.12262382358312607, 0.11024413257837296, 0.11345117539167404, 0.11215195059776306,
 0.11132611334323883, 0.11107751727104187, 0.10451696068048477]
```

```elixir
data = [num_clusters: nums_clusters, silhouette_scores: silhouette_scores]

Vl.new(width: 600, height: 300, title: "Silhouette score vs Number of Clusters")
|> Vl.data_from_values(data)
|> Vl.layers([
  Vl.new()
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "num_clusters",
    title: "Number of Clusters",
    type: :ordinal,
    axis: [label_angle: 0]
  )
  |> Vl.encode_field(:y, "silhouette_scores",
    title: "Silhouette score",
    type: :quantitative,
    scale: [domain: [0.088, 0.205]]
  ),
  Vl.new()
  |> Vl.mark(:circle, size: 50, color: :dark_blue)
  |> Vl.encode_field(:x, "num_clusters")
  |> Vl.encode_field(:y, "silhouette_scores", type: :quantitative)
])
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

As we can see, the model with num_clusters equal to 3 has the highest Silhouette Score. Now we will visualize this clusterization.

```elixir
best_num_clusters = 3
best_model = Enum.at(models, 1)
```

<!-- livebook:{"output":true} -->

```
%Scholar.Cluster.KMeans{
  num_iterations: #Nx.Tensor<
    s64
    EXLA.Backend<host:0, 0.3940695604.3472228364.42080>
    3
  >,
  clusters: #Nx.Tensor<
    f32[3][784]
    EXLA.Backend<host:0, 0.3940695604.3472228364.42126>
    [
      [0.0, 5.9417710872367024e-5, 1.1883542174473405e-4, 2.376708434894681e-4, 2.376708434894681e-4, 4.753416869789362e-4, 2.376708434894681e-4, 0.013071895577013493, 0.05971479415893555, 0.12269756942987442, 0.27730244398117065, 0.3171122968196869, 0.2941770851612091, 0.2795603275299072, 0.28009507060050964, 0.31200236082077026, 0.2995246648788452, 0.3170528709888458, 0.25864526629447937, 0.07664884626865387, 0.02192513458430767, 1.7825313261710107e-4, 4.1592397610656917e-4, 3.565062361303717e-4, 1.1883542174473405e-4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.376708434894681e-4, 5.9417710872367024e-5, 0.009566251188516617, 0.05246583744883537, 0.11509210616350174, 0.23559121787548065, 0.3770647943019867, 0.5828877091407776, 0.6433154940605164, 0.7103387117385864, 0.7102198004722595, 0.6955437064170837, 0.7320261001586914, 0.675638735294342, 0.6002377271652222, 0.5515151619911194, 0.35151517391204834, ...],
      ...
    ]
  >,
  inertia: #Nx.Tensor<
    f32
    EXLA.Backend<host:0, 0.3940695604.3472228364.42128>
    9246.3125
  >,
  labels: #Nx.Tensor<
    s64[200]
    EXLA.Backend<host:0, 0.3940695604.3472228364.42130>
    [0, 0, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 0, 1, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 2, 1, 1, 1, 0, 0, 2, 0, 2, 1, ...]
  >
}
```

```elixir
predicted_cluster_with_indices =
  best_model.labels
  |> Nx.to_flat_list()
  |> Enum.with_index()
  |> Enum.group_by(&elem(&1, 0), &elem(&1, 1))

for cluster <- 0..(best_num_clusters - 1) do
  indices = predicted_cluster_with_indices[cluster]

  boxes =
    for index <- indices do
      original_cluster = Nx.to_number(target[index])

      Kino.Layout.grid([
        Kino.Markdown.new("Original cluster: #{original_cluster}"),
        tensor_to_kino.(images[index])
      ])
    end

  Kino.Layout.grid(
    [
      Kino.Markdown.new("## Cluster #{cluster}"),
      Kino.Layout.grid(boxes, columns: 5)
    ],
    boxed: true
  )
end
|> Kino.Layout.grid()
```

Oops, it doesn't look right! That's because our algorithm for three clusters gathers images by colors rather than shapes. To spot this, let's plot the average image of each cluster.

```elixir
for cluster <- 0..(best_num_clusters - 1) do
  indices = predicted_cluster_with_indices[cluster]

  mean_image =
    indices
    |> Enum.map(&images[&1])
    |> Nx.stack()
    |> Nx.mean(axes: [0])

  tensor_to_kino.(mean_image)
end
|> Kino.Layout.grid(columns: 3)
```

One of the images has a vertical line (something like trousers), the next image is almost all white (similar to a jumper), and the last one is mostly black. This time Silhouette Score turns out to be not the best indicator. To get better clustering, try to rerun the code with a higher number of clusters.
