<!-- livebook:{"persist_outputs":true} -->

# Linear Regression in Practice

```elixir
Mix.install([
  {:scholar, "~> 0.1"},
  {:nx, "~> 0.5.1"},
  {:explorer, "~> 0.5.1"},
  {:exla, "~> 0.5.0"},
  {:req, "~> 0.3.3"},
  {:kino_vega_lite, "~> 0.1.7"},
  {:kino, "~> 0.8.1"}
])
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Introduction

In the livebook, we will cover the typical use cases of linear regression on practical examples.

```elixir
alias VegaLite, as: Vl
require Explorer.DataFrame, as: DF
require Explorer.Series, as: S
Nx.global_default_backend(EXLA.Backend)
seed = 42
```

<!-- livebook:{"output":true} -->

```
42
```

<!-- livebook:{"branch_parent_index":0} -->

## Linear Regression on Synthetic Data

Before we dive into real-life use cases of linear regression, we start with a simpler one. We will generate data with a linear pattern and then use `Scholar.Linear.LinearRegression` to compute regression.

Firstly, we generate the data which simulates the function $f(x) = 3x + 4$ with added uniform, zero-mean noise. `Nx.Random.uniform` creates a tensor with a given shape and type.

```elixir
defmodule LinearData do
  import Nx.Defn

  defn data do
    key = Nx.Random.key(42)
    size = 100
    {x, new_key} = Nx.Random.uniform(key, 0, 2, shape: {size, 1}, type: :f64)
    {noise, _} = Nx.Random.uniform(new_key, -0.5, 0.5, shape: {size, 1}, type: :f64)
    y = 3 * x + 4 + noise
    {x, y}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, LinearData, <<70, 79, 82, 49, 0, 0, 11, ...>>, true}
```

Now let's plot the generated points.

```elixir
size = 100
{x, y} = LinearData.data()

df = DF.new(x: x, y: y)

Vl.new(
  title: [
    text: "Scatterplot of Generated Data",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.data_from_values(df)
|> Vl.mark(:circle)
|> Vl.encode_field(:x, "x",
  type: :quantitative,
  scale: [domain: [-0.05, 2.05]],
  axis: [grid: false]
)
|> Vl.encode_field(:y, "y", type: :quantitative, scale: [domain: [2.5, 12]], axis: [grid: false])
```

<!-- livebook:{"output":true} -->

```

23:13:52.540 [info] TfrtCpuClient created.

```

For a regression task, we will use the `Scholar.Linear.LinearRegression` module.

```elixir
model = Scholar.Linear.LinearRegression.fit(x, y)
```

<!-- livebook:{"output":true} -->

```
%Scholar.Linear.LinearRegression{
  coefficients: #Nx.Tensor<
    f64[1][1]
    EXLA.Backend<host:0, 0.1766168485.4260495372.100751>
    [
      [2.9578183089017]
    ]
  >,
  intercept: #Nx.Tensor<
    f64[1]
    EXLA.Backend<host:0, 0.1766168485.4260495372.100755>
    [4.023067929858136]
  >
}
```

As we can see, the coefficient is almost 3.0, and the intercept is nearly 4.0. Those are decent estimations. They are not exactly equal to 3.0 and 4.0 because we introduce noise to our samples.

Now, let's plot the result of linear regression.

```elixir
[intercept] = Nx.to_flat_list(model.intercept)
[coefficients] = Nx.to_flat_list(model.coefficients)
x_1 = 0
x_2 = 2

line = %{
  x: [x_1, x_2],
  y: [x_1 * coefficients + intercept, x_2 * coefficients + intercept]
}

Vl.new(
  title: [
    text: "Scatterplot of Generated Data",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values(df)
  |> Vl.mark(:circle)
  |> Vl.encode_field(:x, "x",
    type: :quantitative,
    scale: [domain: [-0.05, 2.05]],
    axis: [grid: false]
  )
  |> Vl.encode_field(:y, "y",
    type: :quantitative,
    scale: [domain: [2.5, 12]],
    axis: [grid: false]
  ),
  Vl.new()
  |> Vl.data_from_values(line)
  |> Vl.mark(:line, color: :green)
  |> Vl.encode_field(:x, "x", type: :quantitative, scale: [domain: [-0.05, 2.05]])
  |> Vl.encode_field(:y, "y", type: :quantitative, scale: [domain: [2.5, 12]])
])
```

Using `Scholar.Linear.LinearRegression.predict`, we can predict an expected value for a given input. However, we must remember that our prediction will be valid only if we consider linearly dependent data. Fortunately, our data set is perfect for this kind of prediction.

Now we will predict one value and draw it on the previous graph in a different color.

```elixir
x_prediction = Nx.tensor([[0.83]])

[y_prediction] =
  model
  |> Scholar.Linear.LinearRegression.predict(x_prediction)
  |> Nx.to_flat_list()

[x_prediction] = Nx.to_flat_list(x_prediction)
{x_prediction, y_prediction}
```

<!-- livebook:{"output":true} -->

```
{0.8299999833106995, 6.478057076882628}
```

```elixir
prediction = %{
  x: [x_prediction],
  y: [y_prediction]
}

Vl.new(
  title: [
    text: "Scatterplot of Generated Data",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values(df)
  |> Vl.mark(:circle)
  |> Vl.encode_field(:x, "x",
    type: :quantitative,
    scale: [domain: [-0.05, 2.05]],
    axis: [grid: false]
  )
  |> Vl.encode_field(:y, "y",
    type: :quantitative,
    scale: [domain: [2.5, 12]],
    axis: [grid: false]
  ),
  Vl.new()
  |> Vl.data_from_values(line)
  |> Vl.mark(:line, color: :green)
  |> Vl.encode_field(:x, "x", type: :quantitative, scale: [domain: [-0.05, 2.05]])
  |> Vl.encode_field(:y, "y", type: :quantitative, scale: [domain: [2.5, 12]]),
  Vl.new()
  |> Vl.data_from_values(prediction)
  |> Vl.mark(:circle, color: :red, size: 80)
  |> Vl.encode_field(:x, "x", type: :quantitative)
  |> Vl.encode_field(:y, "y", type: :quantitative)
])
```

As we expected, the red dot lies on the regression line.

This implementation of linear regression is based on the so-called *Least Squares* method. In practice, the function computes $X^+y$ where $X^+$ is a pseudo-inverse matrix (more precisely, Moore-Penrose matrix). You can calculate the results using `Nx.LinAlg.pinv/2`.

```elixir
x_b = Nx.concatenate([Nx.broadcast(1.0, {size, 1}), x], axis: 1)
x_b |> Nx.LinAlg.pinv() |> Nx.dot(y)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f64[2][1]
  EXLA.Backend<host:0, 0.1766168485.4260495371.99138>
  [
    [4.023067929858136],
    [2.957818308901698]
  ]
>
```

We are ready to dive into a more complex example!

<!-- livebook:{"branch_parent_index":0} -->

## California Housing

In this section we will play with [California Housing Data Set](https://www.kaggle.com/datasets/camnugent/california-housing-prices). The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned, so there are some preprocessing steps required! The columns are as follows (their names are pretty self explanatory):

* `longitude`
* `latitude`
* `housing_median_age`
* `total_rooms`
* `total_bedrooms`
* `population`
* `households`
* `median_income`
* `median_house_value`
* `ocean_proximity`

The main task of this section is to predict the median_house_income. However, before we use our linear regression for prediction, we need to learn more about the data.

```elixir
data =
  Req.get!(
    "https://raw.githubusercontent.com/sonarsushant/California-House-Price-Prediction/master/housing.csv"
  ).body

df = DF.load_csv!(data)
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[20640 x 10]
  longitude float [-122.23, -122.22, -122.24, -122.25, -122.25, ...]
  latitude float [37.88, 37.86, 37.85, 37.85, 37.85, ...]
  housing_median_age float [41.0, 21.0, 52.0, 52.0, 52.0, ...]
  total_rooms float [880.0, 7099.0, 1467.0, 1274.0, 1627.0, ...]
  total_bedrooms float [129.0, 1106.0, 190.0, 235.0, 280.0, ...]
  population float [322.0, 2401.0, 496.0, 558.0, 565.0, ...]
  households float [126.0, 1138.0, 177.0, 219.0, 259.0, ...]
  median_income float [8.3252, 8.3014, 7.2574, 5.6431, 3.8462, ...]
  median_house_value float [4.526e5, 3.585e5, 3.521e5, 3.413e5, 3.422e5, ...]
  ocean_proximity string ["NEAR BAY", "NEAR BAY", "NEAR BAY", "NEAR BAY", "NEAR BAY", ...]
>
```

Firstly, let's look at the distribution of houses based on the distance to the ocean.

```elixir
S.frequencies(df["ocean_proximity"])
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[5 x 2]
  values string ["<1H OCEAN", "INLAND", "NEAR OCEAN", "NEAR BAY", "ISLAND"]
  counts integer [9136, 6551, 2658, 2290, 5]
>
```

Now, we will plot univariate histograms for each feature of the data set.

```elixir
# Increase the sample size (or use 1.0 to plot all data)
sample = DF.sample(df, 0.2, seed: seed)

Vl.new(
  title: [
    text: "Univariate Histograms of all features",
    anchor: :middle
  ],
  width: 500,
  height: 500,
  columns: 3
)
|> Vl.data_from_values(sample)
|> Vl.concat(
  for name <- List.delete(df.names, "ocean_proximity") do
    Vl.new()
    |> Vl.mark(:bar)
    |> Vl.encode_field(:x, name, bin: [bin: true, maxbins: 50], axis: [ticks: false])
    |> Vl.encode_field(:y, "value count", aggregate: :count)
  end
)
```

From histograms, we can spot that *median_income* and *median_house_values* have a similar distribution. Both of them are heavy-tailed with high skewness. We might speculate that those two features are strictly correlated. We will check that later on.

<!-- livebook:{"break_markdown":true} -->

Now let's render the houses as a scatter plot using the latitude and longitude, overlayed on top of California's map. Let's also use color to encode the house prices and use the circle size to indicate the population of districts.

```elixir
Vl.new(
  title: [
    text: "Scatterplot of Generated Data",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values([
    %{
      url:
        "https://raw.githubusercontent.com/ageron/handson-ml2/master/images/end_to_end_project/california.png"
    }
  ])
  |> Vl.mark(:image,
    width: 630,
    aspect: false,
    align: :right
  )
  |> Vl.encode_field(:url, "url", type: :nominal),
  Vl.new()
  |> Vl.data_from_values(df, only: ~w(latitude longitude median_house_value population))
  |> Vl.mark(:circle, opacity: 0.4)
  |> Vl.encode_field(:x, "longitude",
    type: :quantitative,
    axis: [grid: false],
    scale: [domain: [-124.55, -113.80]]
  )
  |> Vl.encode_field(:y, "latitude",
    type: :quantitative,
    axis: [grid: false],
    scale: [domain: [32.45, 42.05]]
  )
  |> Vl.encode_field(:color, "median_house_value", type: :quantitative, scale: [scheme: :viridis])
  |> Vl.encode_field(:size, "population", type: :quantitative)
])
```

From This plot, we can read that prices are substantially dependent on **geolocalization** and **population**. For geolocalization, we see, those areas closer to the ocean are more expensive. But it's not a strict rule since houses on the northern bay of California are much more affordable than in in-land Mid California. For the population, there are two dense areas with expensive housing: Los Angeles Bay (In South California) and San Francisco Bay (in Mid Califonia). They are metropolises with a lot of different tech companies, and business and cultural institutions, so, logically, housing in those places will be expensive.

<ins>
<i>
Hint:
</i>
</ins>

<br />

You can try to add another feature by computing clustering on this data set. It might be a sum or power mean of distances to the clusters. We may predict that centroids will be located in San Francisco Bay and Los Angeles Bay. You can also pass population as weights to k-means.

<!-- livebook:{"break_markdown":true} -->

Before we convert our data to tensor, we will add three more columns which might be informative:

* `rooms_per_family`
* `bedrooms_per_rooms`
* `population_per_family`

The names of columns are self-describing. Now, add them to our data frame.

```elixir
df =
  DF.mutate(df,
    rooms_per_family: total_rooms / households,
    bedrooms_per_rooms: total_bedrooms / total_rooms,
    population_per_family: population / households
  )
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[20640 x 13]
  longitude float [-122.23, -122.22, -122.24, -122.25, -122.25, ...]
  latitude float [37.88, 37.86, 37.85, 37.85, 37.85, ...]
  housing_median_age float [41.0, 21.0, 52.0, 52.0, 52.0, ...]
  total_rooms float [880.0, 7099.0, 1467.0, 1274.0, 1627.0, ...]
  total_bedrooms float [129.0, 1106.0, 190.0, 235.0, 280.0, ...]
  population float [322.0, 2401.0, 496.0, 558.0, 565.0, ...]
  households float [126.0, 1138.0, 177.0, 219.0, 259.0, ...]
  median_income float [8.3252, 8.3014, 7.2574, 5.6431, 3.8462, ...]
  median_house_value float [4.526e5, 3.585e5, 3.521e5, 3.413e5, 3.422e5, ...]
  ocean_proximity string ["NEAR BAY", "NEAR BAY", "NEAR BAY", "NEAR BAY", "NEAR BAY", ...]
  rooms_per_family float [6.984126984126984, 6.238137082601054, 8.288135593220339,
   5.8173515981735155, 6.281853281853282, ...]
  bedrooms_per_rooms float [0.14659090909090908, 0.15579659106916466, 0.12951601908657123,
   0.18445839874411302, 0.1720958819913952, ...]
  population_per_family float [2.5555555555555554, 2.109841827768014, 2.8022598870056497,
   2.547945205479452, 2.1814671814671813, ...]
>
```

In the next step, we will find the correlation matrix. But to do this, we need to cast our data frame to Nx tensor and split data into train and test sets.

First let's remove all `:nan`s from our data and then convert the "ocean_proximity" string column to a numerical one. Explorer supports a `:category` type but, in this case, we will do a custom conversion as we can consider the ocean proximity as ordinal data since we can order them by the distance to the ocean. The bigger value the further from the ocean.

```elixir
# Replace all nils with :nan so we are able to convert to tensor.
names =
  df
  |> DF.names()
  |> List.delete("ocean_proximity")

after_preprocessing = for name <- names, into: %{}, do: {name, S.fill_missing(df[name], :nan)}

preprocessed_data = DF.new(after_preprocessing)

mapping = %{
  "ISLAND" => 0.0,
  "<1H OCEAN" => 1.0,
  "NEAR OCEAN" => 2.0,
  "NEAR BAY" => 3.0,
  "INLAND" => 4.0
}

mapped_location = S.transform(df["ocean_proximity"], fn x -> Map.fetch!(mapping, x) end)

df = DF.put(preprocessed_data, :ocean_proximity, mapped_location)
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[20640 x 13]
  bedrooms_per_rooms float [0.14659090909090908, 0.15579659106916466, 0.12951601908657123,
   0.18445839874411302, 0.1720958819913952, ...]
  households float [126.0, 1138.0, 177.0, 219.0, 259.0, ...]
  housing_median_age float [41.0, 21.0, 52.0, 52.0, 52.0, ...]
  latitude float [37.88, 37.86, 37.85, 37.85, 37.85, ...]
  longitude float [-122.23, -122.22, -122.24, -122.25, -122.25, ...]
  median_house_value float [4.526e5, 3.585e5, 3.521e5, 3.413e5, 3.422e5, ...]
  median_income float [8.3252, 8.3014, 7.2574, 5.6431, 3.8462, ...]
  population float [322.0, 2401.0, 496.0, 558.0, 565.0, ...]
  population_per_family float [2.5555555555555554, 2.109841827768014, 2.8022598870056497,
   2.547945205479452, 2.1814671814671813, ...]
  rooms_per_family float [6.984126984126984, 6.238137082601054, 8.288135593220339,
   5.8173515981735155, 6.281853281853282, ...]
  total_bedrooms float [129.0, 1106.0, 190.0, 235.0, 280.0, ...]
  total_rooms float [880.0, 7099.0, 1467.0, 1274.0, 1627.0, ...]
  ocean_proximity float [3.0, 3.0, 3.0, 3.0, 3.0, ...]
>
```

Now we convert dataframes into tensors. We can do so by concatenating and stacking the columns accordingly:

```elixir
# Shuffle data to make splitting more resonable
{num_rows, _num_cols} = DF.shape(df)

indices = Nx.iota({num_rows})
key = Nx.Random.key(42)
{permutation_indices, _} = Nx.Random.shuffle(key, Nx.iota({num_rows}), axis: 0)

y =
  df[["median_house_value"]]
  |> Nx.concatenate()
  |> Nx.take(permutation_indices)

x =
  df
  |> DF.discard("median_house_value")
  |> Nx.stack(axis: 1)
  |> Nx.take(permutation_indices)

{x, y}
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   f64[20640][12]
   EXLA.Backend<host:0, 0.1766168485.4260495371.100323>
   [
     [0.2707641196013289, 326.0, 15.0, 32.76, -117.02, 1.0278, 543.0, 1.665644171779141, 3.6932515337423313, 326.0, 1204.0, 1.0],
     [0.2598343685300207, 230.0, 27.0, 38.44, -122.71, 1.7, 462.0, 2.008695652173913, 4.2, 251.0, 966.0, 1.0],
     [0.2902033271719039, 793.0, 35.0, 34.09, -118.35, 3.0349, 1526.0, 1.9243379571248425, 3.41109709962169, 785.0, 2705.0, 1.0],
     [0.2940552016985138, 549.0, 25.0, 33.91, -118.35, 2.8512, 1337.0, 2.435336976320583, 3.431693989071038, 554.0, 1884.0, 1.0],
     [0.2067861321509945, ...],
     ...
   ]
 >,
 #Nx.Tensor<
   f64[20640]
   EXLA.Backend<host:0, 0.1766168485.4260495371.100294>
   [1.542e5, 3.5e5, 2.667e5, 2.728e5, 1.163e5, 2.941e5, 4.178e5, 1.851e5, 5.68e4, 500001.0, 500001.0, 1.152e5, 2.343e5, 8.75e4, 8.37e4, 500001.0, 1.669e5, 3.003e5, 1.546e5, 6.69e4, 2.325e5, 1.367e5, 1.375e5, 1.27e5, 1.683e5, 1.441e5, 9.53e4, 1.834e5, 1.435e5, 6.85e4, 1.625e5, 1.661e5, 1.908e5, 2.431e5, 1.488e5, 3.036e5, 1.479e5, 1.5e5, 3.288e5, 7.08e4, 2.25e5, 1.375e5, 3.5e5, 3.742e5, 1.549e5, 4.5e5, 3.063e5, 2.051e5, ...]
 >}
```

Since we don't have a stratified split of data implemented (to learn more see [Stratified Sampling](https://en.wikipedia.org/wiki/Stratified_sampling)), we shuffle data set and take advantage of [Law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). It says that the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed. As we take a lot of samples from shuffled data it implies that the sampled data sets will be stratified. Now, we will split the data into training and test sets.

```elixir
train_ratio = 0.8
num_train = round(train_ratio * num_rows)

{x_train, x_test} = {x[[0..(num_train - 1), ..]], x[[num_train..(num_rows - 1), ..]]}
{y_train, y_test} = {y[[0..(num_train - 1)]], y[[num_train..(num_rows - 1)]]}
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   f64[16512]
   EXLA.Backend<host:0, 0.1766168485.4260495371.100329>
   [1.542e5, 3.5e5, 2.667e5, 2.728e5, 1.163e5, 2.941e5, 4.178e5, 1.851e5, 5.68e4, 500001.0, 500001.0, 1.152e5, 2.343e5, 8.75e4, 8.37e4, 500001.0, 1.669e5, 3.003e5, 1.546e5, 6.69e4, 2.325e5, 1.367e5, 1.375e5, 1.27e5, 1.683e5, 1.441e5, 9.53e4, 1.834e5, 1.435e5, 6.85e4, 1.625e5, 1.661e5, 1.908e5, 2.431e5, 1.488e5, 3.036e5, 1.479e5, 1.5e5, 3.288e5, 7.08e4, 2.25e5, 1.375e5, 3.5e5, 3.742e5, 1.549e5, 4.5e5, 3.063e5, 2.051e5, 2.737e5, ...]
 >,
 #Nx.Tensor<
   f64[4128]
   EXLA.Backend<host:0, 0.1766168485.4260495371.100331>
   [8.55e4, 1.743e5, 5.65e4, 1.308e5, 1.375e5, 2.472e5, 2.25e5, 1.164e5, 500001.0, 1.154e5, 1.269e5, 1.229e5, 7.04e4, 3.534e5, 7.14e4, 4.083e5, 3.0e5, 1.648e5, 1.125e5, 2.028e5, 1.139e5, 2.15e5, 500001.0, 1.48e5, 1.683e5, 2.819e5, 3.338e5, 1.078e5, 3.277e5, 3.412e5, 3.289e5, 2.879e5, 2.545e5, 2.667e5, 6.82e4, 1.406e5, 2.795e5, 2.25e5, 1.17e5, 1.375e5, 3.115e5, 2.423e5, 4.93e5, 3.458e5, 1.542e5, 3.728e5, 6.6e4, 1.281e5, ...]
 >}
```

Before we compute the correlation matrix, we will check if we have NaNs (Not a Number) in the data set.

```elixir
y_nan_count = Nx.sum(Nx.is_nan(y))
x_nan_count = Nx.sum(Nx.is_nan(x))
{x_nan_count, y_nan_count}
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   u64
   EXLA.Backend<host:0, 0.1766168485.4260495371.100339>
   414
 >,
 #Nx.Tensor<
   u64
   EXLA.Backend<host:0, 0.1766168485.4260495371.100335>
   0
 >}
```

Ups, we have some. Fortunately, for y, we don't have any NaNs. If we dig a little bit more, it turns out that NaNs are in <pre> bedrooms_per_rooms (1st row) </pre>  <pre> total_bedrooms (10th row) </pre>

```elixir
{bedrooms_per_rooms_idx, total_bedrooms_idx} = {0, 9}
bedrooms_per_rooms_nan_count = Nx.sum(Nx.is_nan(x[[.., bedrooms_per_rooms_idx]]))
total_bedrooms_nan_count = Nx.sum(Nx.is_nan(x[[.., total_bedrooms_idx]]))
Nx.equal(x_nan_count, Nx.add(bedrooms_per_rooms_nan_count, total_bedrooms_nan_count))
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  u8
  EXLA.Backend<host:0, 0.1766168485.4260495371.100354>
  1
>
```

For these two, we use `Scholar.Impute.SimpleImputer` with startegy set to median of values. Function `fit` learn the median of features and `transform` for trained model replace all NaNs with a given startegy. It is important that we perform imputation after splitting data because otherwise we will have a leakage of information from test data.

```elixir
x_train =
  x_train
  |> Scholar.Impute.SimpleImputer.fit(strategy: :median)
  |> Scholar.Impute.SimpleImputer.transform(x_train)

x_test =
  x_test
  |> Scholar.Impute.SimpleImputer.fit(strategy: :median)
  |> Scholar.Impute.SimpleImputer.transform(x_test)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f64[4128][12]
  EXLA.Backend<host:0, 0.1766168485.4260495371.100438>
  [
    [0.20019126554032515, 548.0, 23.0, 36.32, -119.33, 2.5, 1446.0, 2.6386861313868613, 5.724452554744525, 628.0, 3137.0, 4.0],
    [0.18461538461538463, 83.0, 34.0, 34.26, -118.44, 5.5124, 433.0, 5.216867469879518, 3.9156626506024095, 60.0, 325.0, 1.0],
    [0.23362175525339926, 398.0, 35.0, 35.77, -119.25, 1.6786, 1449.0, 3.64070351758794, 4.065326633165829, 378.0, 1618.0, 4.0],
    [0.20259128386336867, 313.0, 24.0, 37.8, -121.2, 3.5625, 927.0, 2.961661341853035, 5.424920127795527, 344.0, 1698.0, 4.0],
    [0.42487046632124353, 155.0, ...],
    ...
  ]
>
```

Eventually, we can compute the correlation matrix. We will use `Scholar.Covariance` to calculate the correlation matrix.

```elixir
correlation =
  Nx.concatenate([x_train, Nx.new_axis(y_train, 1)], axis: 1)
  |> Scholar.Covariance.correlation_matrix(biased: true)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f64[13][13]
  EXLA.Backend<host:0, 0.1766168485.4260495371.100472>
  [
    [1.0000000000000002, 0.06144282875654085, 0.13311283521287637, -0.12216156759996018, 0.09984860365269956, -0.612630933680883, 0.03517997111673592, 0.004697459810143742, -0.4078383723411937, 0.08095666109618091, -0.18612145325855575, -0.11835385450623682, -0.25385512001635363],
    [0.06144282875654085, 1.0, -0.305555948690877, -0.07566125267999342, 0.06170866872453172, 0.01398821983308101, 0.907460734474932, -0.02634672373043427, -0.07388109216765114, 0.9735036322842058, 0.9198029415934222, -0.04626792658479704, 0.0639675034251392],
    [0.13311283521287637, -0.305555948690877, 0.9999999999999998, 0.017212690983879612, -0.11382253728911094, -0.12557649464959478, -0.2961765721596695, 0.012442541870155687, -0.14834326174239934, -0.32094539339262423, -0.3602627671081426, -0.11940626101050672, 0.10251505008793772],
    [-0.12216156759996018, -0.07566125267999342, 0.017212690983879612, 0.9999999999999992, -0.92526150146814, -0.0709535112221358, -0.11317680203176562, 0.0032929374670018535, 0.10562463526360089, -0.07174931649052743, -0.038747214812891013, ...],
    ...
  ]
>
```

Maybe visual representation would be nicer. ðŸ˜…

```elixir
{corr_size, _} = Nx.shape(correlation)
correlation_list = Nx.to_flat_list(correlation)

names = [
  "Bedrooms per rooms",
  "Households",
  "Housing median age",
  "Latitude",
  "Longitude",
  "Median income",
  "Population",
  "Population per family",
  "Rooms per family",
  "Total bedrooms",
  "Total rooms",
  "Ocean proximity",
  "Median house value"
]

corr_to_plot =
  DF.new(
    x: List.flatten(List.duplicate(names, corr_size)),
    y: List.flatten(for name <- names, do: List.duplicate(name, corr_size)),
    corr_val: Enum.map(correlation_list, fn x -> Float.round(x, 2) end)
  )

Vl.new(
  title: [
    text: "Correlation Matrix for California Housing",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.data_from_values(corr_to_plot)
|> Vl.layers([
  Vl.new()
  |> Vl.mark(:rect)
  |> Vl.encode_field(:x, "x", type: :nominal, title: "", sort: false)
  |> Vl.encode_field(:y, "y", type: :nominal, title: "", sort: false)
  |> Vl.encode_field(:color, "corr_val", type: :quantitative, scale: [scheme: :viridis]),
  Vl.new()
  |> Vl.mark(:text)
  |> Vl.encode_field(:x, "x", type: :nominal, title: "")
  |> Vl.encode_field(:y, "y", type: :nominal, title: "")
  |> Vl.encode_field(:text, "corr_val", type: :quantitative)
  |> Vl.encode_field(:color, "corr_val",
    type: :quantitative,
    condition: [
      [test: "datum['corr_val'] < 0", value: :white],
      [test: "datum['corr_val'] >= 0", value: :black]
    ]
  )
])
```

We can spot that _median_house_value_ is strongly correlated with _median_income_. It's pretty straightforward, the more money you have, the more expensive house you can buy. Non-obvious is a negative correlation with _bedrooms_per_rooms_. But it also can be explained. Bedrooms are the most crucial rooms in the house. Firstly, you need to guarantee that you have a house with enough bedrooms. If this condition is satisfied, then you can focus on "additional rooms" like a chill room, cabinets and so on. So if you buy a house with more additional rooms, then you decrease the ratio.

<!-- livebook:{"break_markdown":true} -->

Now we are ready to train a model for the _median_house_value_ prediction. We will use linear regression. In the first step, we create the model by calling the `fit` function.

```elixir
model = Scholar.Linear.LinearRegression.fit(x_train, y_train)
```

<!-- livebook:{"output":true} -->

```
%Scholar.Linear.LinearRegression{
  coefficients: #Nx.Tensor<
    f64[12]
    EXLA.Backend<host:0, 0.1766168485.4260495371.100999>
    [292661.1879978386, 111.46237796967083, 1170.1666131366273, -33322.29721781791, -35394.90464853701, 41384.64324873476, -41.31598131072877, 50.507900596456054, 3040.877192253343, 7.253624678472774, 3.272614960520949, -8059.519389730254]
  >,
  intercept: #Nx.Tensor<
    f64
    EXLA.Backend<host:0, 0.1766168485.4260495371.101003>
    -3101555.563125743
  >
}
```

Now we can predict the values for the test set and measure the error of our prediction. We will calculate root mean square error (RMSE) and mean absolute error (MAE).

```elixir
predictions = Scholar.Linear.LinearRegression.predict(model, x_test)
rmse = Scholar.Metrics.mean_square_error(y_test, predictions) |> Nx.sqrt()
mae = Scholar.Metrics.mean_absolute_error(y_test, predictions)
{rmse, mae}
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   f64
   EXLA.Backend<host:0, 0.1766168485.4260495371.101015>
   67648.9435367406
 >,
 #Nx.Tensor<
   f64
   EXLA.Backend<host:0, 0.1766168485.4260495371.101020>
   48942.11908533544
 >}
```

Ok, but is it a good or poor estimation? Huh, check the mean value of the target and then compare it to the value of errors.

```elixir
Nx.mean(y)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f64
  EXLA.Backend<host:0, 0.1766168485.4260495371.101023>
  206855.81690891474
>
```

For such a simple model as linear regression, it seems to be a pretty good result. But there is space to improve this result. You can, for example, add some additional features to the data set. In the future, you will be able to try more complicated models, such as random forests.
