# Linear Regression in Practice

```elixir
Mix.install(
  [
    {:scholar, github: "elixir-nx/scholar"},
    {:nx, "~> 0.5.0", override: true},
    {:explorer, "~> 0.5.0"},
    {:exla, "~> 0.5.0"},
    {:req, "~> 0.3.3"},
    {:vega_lite, "~> 0.1.6"},
    {:kino_vega_lite, "~> 0.1.7"},
    {:kino, "~> 0.8.1"}
  ]
)
```

## Introduction

In the livebook, we will cover the typical use cases of linear regression on practical examples.

```elixir
alias VegaLite, as: Vl
require Explorer.DataFrame, as: DF
require Explorer.Series, as: S
Nx.global_default_backend(EXLA.Backend)
```

<!-- livebook:{"branch_parent_index":0} -->

## Linear Regression on Synthetic Data

Before we dive into real-life use cases of linear regression, we start with a simpler one. We will generate data with a linear pattern and then use `Scholar.Linear.LinearRegression` to compute regression.

Firstly, we generate the data which simulates the function $f(x) = 3x + 4$ with added uniform, zero-mean noise. `Nx.Random.uniform` creates a tensor with a given shape and type.

```elixir
defmodule LinearData do
  import Nx.Defn

  defn data do
    key = Nx.Random.key(42)
    size = 100
    {x, new_key} = Nx.Random.uniform(key, 0, 2, shape: {size, 1}, type: :f64)
    {noise, _} = Nx.Random.uniform(new_key, -0.5, 0.5, shape: {size, 1}, type: :f64)
    y = 3 * x + 4 + noise
    {x, y}
  end
end
```

Now let's plot the generated points.

```elixir
size = 100
{x, y} = LinearData.data()

df =
  DF.new([])
  |> DF.put(:x, x)
  |> DF.put(:y, y)

Vl.new(
  title: [
    text: "Scatterplot of Generated Data",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.data_from_values(df)
|> Vl.mark(:circle)
|> Vl.encode_field(:x, "x",
  type: :quantitative,
  scale: [domain: [-0.05, 2.05]],
  axis: [grid: false]
)
|> Vl.encode_field(:y, "y", type: :quantitative, scale: [domain: [2.5, 12]], axis: [grid: false])
```

For a regression task, we will use the `Scholar.Linear.LinearRegression` module.

```elixir
model = Scholar.Linear.LinearRegression.fit(x, y)
```

As we can see, the coefficient is almost 3.0, and the intercept is nearly 4.0. Those are decent estimations. They are not exactly equal to 3.0 and 4.0 because we introduce noise to our samples.

Now, let's plot the result of linear regression.

```elixir
[intercept] = Nx.to_flat_list(model.intercept)
[coefficients] = Nx.to_flat_list(model.coefficients)
x_1 = 0
x_2 = 2

line = %{
  x: [x_1, x_2],
  y: [x_1 * coefficients + intercept, x_2 * coefficients + intercept]
}

Vl.new(
  title: [
    text: "Scatterplot of Generated Data",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values(df)
  |> Vl.mark(:circle)
  |> Vl.encode_field(:x, "x",
    type: :quantitative,
    scale: [domain: [-0.05, 2.05]],
    axis: [grid: false]
  )
  |> Vl.encode_field(:y, "y",
    type: :quantitative,
    scale: [domain: [2.5, 12]],
    axis: [grid: false]
  ),
  Vl.new()
  |> Vl.data_from_values(line)
  |> Vl.mark(:line, color: :green)
  |> Vl.encode_field(:x, "x", type: :quantitative, scale: [domain: [-0.05, 2.05]])
  |> Vl.encode_field(:y, "y", type: :quantitative, scale: [domain: [2.5, 12]])
])
```

Using `Scholar.Linear.LinearRegression.predict`, we can predict an expected value for a given input. However, we must remember that our prediction will be valid only if we consider linearly dependent data. Fortunately, our data set is perfect for this kind of prediction.

Now we will predict one value and draw it on the previous graph in a different color.

```elixir
x_prediction = Nx.tensor([[0.83]])

[y_prediction] =
  model
  |> Scholar.Linear.LinearRegression.predict(x_prediction)
  |> Nx.to_flat_list()

[x_prediction] = Nx.to_flat_list(x_prediction)
{x_prediction, y_prediction}
```

```elixir
prediction =
  %{
    x: [x_prediction],
    y: [y_prediction]
  }

Vl.new(
  title: [
    text: "Scatterplot of Generated Data",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values(df)
  |> Vl.mark(:circle)
  |> Vl.encode_field(:x, "x",
    type: :quantitative,
    scale: [domain: [-0.05, 2.05]],
    axis: [grid: false]
  )
  |> Vl.encode_field(:y, "y",
    type: :quantitative,
    scale: [domain: [2.5, 12]],
    axis: [grid: false]
  ),
  Vl.new()
  |> Vl.data_from_values(line)
  |> Vl.mark(:line, color: :green)
  |> Vl.encode_field(:x, "x", type: :quantitative, scale: [domain: [-0.05, 2.05]])
  |> Vl.encode_field(:y, "y", type: :quantitative, scale: [domain: [2.5, 12]]),
  Vl.new()
  |> Vl.data_from_values(prediction)
  |> Vl.mark(:circle, color: :red, size: 80)
  |> Vl.encode_field(:x, "x", type: :quantitative)
  |> Vl.encode_field(:y, "y", type: :quantitative)
])
```

As we expected, the red dot lies on the regression line.

This implementation of linear regression is based on the so-called *Least Squares* method. In practice, the function computes $X^+y$ where $X^+$ is a pseudo-inverse matrix (more precisely, Moore-Penrose matrix). You can calculate the results using `Nx.LinAlg.pinv/2`.

```elixir
x_b = Nx.concatenate([Nx.broadcast(1.0, {size, 1}), x], axis: 1)
x_b |> Nx.LinAlg.pinv() |> Nx.dot(y)
```

We are ready to dive into a more complex example!

<!-- livebook:{"branch_parent_index":0} -->

## California Housing

In this section we will play with [California Housing Data Set](https://www.kaggle.com/datasets/camnugent/california-housing-prices). The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned, so there are some preprocessing steps required! The columns are as follows (their names are pretty self explanatory):

* `longitude`
* `latitude`
* `housing_median_age`
* `total_rooms`
* `total_bedrooms`
* `population`
* `households`
* `median_income`
* `median_house_value`
* `ocean_proximity`

The main task of this section is to predict the median_house_income. However, before we use our linear regression for prediction, we need to learn more about the data.

```elixir
data =
  Req.get!(
    "https://raw.githubusercontent.com/sonarsushant/California-House-Price-Prediction/master/housing.csv"
  ).body

df = DF.load_csv!(data)
```

Firstly, let's look at the distribution of houses based on the distance to the ocean.

```elixir
S.frequencies(df["ocean_proximity"])
```

Now, we will plot univariate histograms for each feature of the data set.

```elixir
Vl.new(
  title: [
    text: "Univariate Histograms of all features"
  ],
  width: 500,
  height: 500,
  columns: 3
)
|> Vl.data_from_values(df)
|> Vl.concat(
  for name <- List.delete(df.names, "ocean_proximity") do
    Vl.new()
    |> Vl.mark(:bar)
    |> Vl.encode_field(:x, name, bin: [bin: true, maxbins: 50], axis: [ticks: false])
    |> Vl.encode_field(:y, "value count", aggregate: :count)
  end
)
```

From histograms, we can spot that *median_income* and *median_house_values* have a similar distribution. Both of them are heavy-tailed with high skewness. We might speculate that those two features are strictly correlated. We will check that later on.

<!-- livebook:{"break_markdown":true} -->

Now, let's plot the distribution of houses on a scatter plot.

```elixir
Vl.new(
  title: [
    text: "Distribution of houses across California",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.data_from_values(df)
|> Vl.mark(:circle)
|> Vl.encode_field(:x, "longitude",
  type: :quantitative,
  axis: [grid: false],
  scale: [domain: [-124.55, -113.80]]
)
|> Vl.encode_field(:y, "latitude",
  type: :quantitative,
  axis: [grid: false],
  scale: [domain: [32.45, 42.05]]
)
```

This plot indeed presents the map of California. But it's hard to spot any pattern in this picture. To make it easier, change the value of the *opacity* parameter to 0.2.

```elixir
Vl.new(
  title: [
    text: "Distribution of houses across California",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.data_from_values(df)
|> Vl.mark(:circle, opacity: 0.2)
|> Vl.encode_field(:x, "longitude",
  type: :quantitative,
  axis: [grid: false],
  scale: [domain: [-124.55, -113.80]]
)
|> Vl.encode_field(:y, "latitude",
  type: :quantitative,
  axis: [grid: false],
  scale: [domain: [32.45, 42.05]]
)
```

We can see better the more dense areas of the plot. Now we will focus more on house prices. Encode them in color on the map. The size of a circle will indicate the population of districts.

```elixir
Vl.new(
  title: [
    text: "Distribution of houses across California",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.data_from_values(df)
|> Vl.mark(:circle, opacity: 0.4)
|> Vl.encode_field(:x, "longitude",
  type: :quantitative,
  axis: [grid: false],
  scale: [domain: [-124.55, -113.80]]
)
|> Vl.encode_field(:y, "latitude",
  type: :quantitative,
  axis: [grid: false],
  scale: [domain: [32.45, 42.05]]
)
|> Vl.encode_field(:color, "median_house_value", type: :quantitative, scale: [scheme: :viridis])
|> Vl.encode_field(:size, "population", type: :quantitative)
```

In the end, we may also add the map of California.

```elixir
Vl.new(
  title: [
    text: "Scatterplot of Generated Data",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values([
    %{
      url:
        "https://raw.githubusercontent.com/ageron/handson-ml2/master/images/end_to_end_project/california.png"
    }
  ])
  |> Vl.mark(:image,
    width: 630,
    aspect: false,
    align: :right
  )
  |> Vl.encode_field(:url, "url", type: :nominal),
  Vl.new()
  |> Vl.data_from_values(df)
  |> Vl.mark(:circle, opacity: 0.4)
  |> Vl.encode_field(:x, "longitude",
    type: :quantitative,
    axis: [grid: false],
    scale: [domain: [-124.55, -113.80]]
  )
  |> Vl.encode_field(:y, "latitude",
    type: :quantitative,
    axis: [grid: false],
    scale: [domain: [32.45, 42.05]]
  )
  |> Vl.encode_field(:color, "median_house_value", type: :quantitative, scale: [scheme: :viridis])
  |> Vl.encode_field(:size, "population", type: :quantitative)
])
```

From This plot, we can read that prices are substantially dependent on **geolocalization** and **population**. For geolocalization, we see, those areas closer to the ocean are more expensive. But it's not a strict rule since houses on the northern bay of California are much more affordable than in in-land Mid California. For the population, there are two dense areas with expensive housing: Los Angeles Bay (In South California) and San Francisco Bay (in Mid Califonia). They are metropolises with a lot of different tech companies, and business and cultural institutions, so, logically, housing in those places will be expensive.

<ins>
<i>
Hint:
</i>
</ins>

<br />

You can try to add another feature by computing clustering on this data set. It might be a sum or power mean of distances to the clusters. We may predict that centroids will be located in San Francisco Bay and Los Angeles Bay. You can also pass population as weights to k-means.

<!-- livebook:{"break_markdown":true} -->

Before we convert our data to tensor, we will add three more columns which might be informative:

* `rooms_per_family`
* `bedrooms_per_rooms`
* `population_per_family`

The names of columns are self-describing. Now, add them to our data frame.

```elixir
df =
  DF.mutate(df,
    rooms_per_family: total_rooms / households,
    bedrooms_per_rooms: total_bedrooms / total_rooms,
    population_per_family: population / households
  )
```

In the next step, we will find the correlation matrix. But to do this, we need to cast our data frame to Nx tensor and split data into train and test sets.

```elixir
# Module to convert Explorer data frames into tensors.
defmodule ToTensor do
  def df_to_matrix(df) do
    df
    |> DF.names()
    |> Enum.map(&S.to_tensor(df[&1]))
    |> Nx.stack(axis: 1)
  end
end
```

```elixir
# Replace all nils with :nan so we are able to convert to tensor.
names =
  df
  |> DF.names()
  |> List.delete("ocean_proximity")

after_preprocessing =
  for name <- names, into: %{}, do: {name, S.fill_missing(df[name], :nan)}

preprocessed_data = DF.new(after_preprocessing)

# Map ocean_proximity from categorical to float
# We can consider this categorical as ordinal data since we can order them by the distance
# to the ocean. The bigger value the further from the ocean.
mapping = %{
  "ISLAND" => 0.0,
  "<1H OCEAN" => 1.0,
  "NEAR OCEAN" => 2.0,
  "NEAR BAY" => 3.0,
  "INLAND" => 4.0
}

mapped_location =
  S.transform(df["ocean_proximity"], fn x -> Map.fetch!(mapping, x) end)

df = DF.put(preprocessed_data, :ocean_proximity, mapped_location)
```

```elixir
# Shuffle data to make splitting more resonable
{num_rows, _num_cols} = DF.shape(df)

indices = Nx.iota({num_rows})
key = Nx.Random.key(42)
{permutation_indices, _} = Nx.Random.shuffle(key, Nx.iota({num_rows}), axis: 0)

y =
  df[["median_house_value"]]
  |> ToTensor.df_to_matrix()
  |> Nx.take(permutation_indices)
  |> Nx.squeeze()

x =
  df
  |> DF.discard("median_house_value")
  |> ToTensor.df_to_matrix()
  |> Nx.take(permutation_indices)

{x, y}
```

Since we don't have a stratified split of data implemented (to learn more see [Stratified Sampling](https://en.wikipedia.org/wiki/Stratified_sampling)), we shuffle data set and take advantage of [Law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). It says that the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed. As we take a lot of samples from shuffled data it implies that the sampled data sets will be stratified. Now, we will split the data into training and test sets.

```elixir
train_ratio = 0.8
num_train = round(train_ratio * num_rows)

{x_train, x_test} = {x[[0..(num_train - 1), ..]], x[[num_train..(num_rows - 1), ..]]}
{y_train, y_test} = {y[[0..(num_train - 1)]], y[[num_train..(num_rows - 1)]]}
```

Before we compute the correlation matrix, we will check if we have NaNs (Not a Number) in the data set.

```elixir
y_nan_count = Nx.sum(Nx.is_nan(y))
x_nan_count = Nx.sum(Nx.is_nan(x))
{x_nan_count, y_nan_count}
```

Ups, we have some. Fortunately, for y, we don't have any NaNs. If we dig a little bit more, it turns out that NaNs are in <pre> bedrooms_per_rooms (1st row) </pre>  <pre> total_bedrooms (10th row) </pre>

```elixir
{bedrooms_per_rooms_idx, total_bedrooms_idx} = {0, 9}
bedrooms_per_rooms_nan_count = Nx.sum(Nx.is_nan(x[[.., bedrooms_per_rooms_idx]]))
total_bedrooms_nan_count = Nx.sum(Nx.is_nan(x[[.., total_bedrooms_idx]]))
Nx.equal(x_nan_count, Nx.add(bedrooms_per_rooms_nan_count, total_bedrooms_nan_count))
```

For these two, we use `Scholar.Impute.SimpleImputer` with startegy set to median of values. Function `fit` learn the median of features and `transform` for trained model replace all NaNs with a given startegy. It is important that we perform imputation after splitting data because otherwise we will have a leakage of information from test data.

```elixir
x_train =
  x_train
  |> Scholar.Impute.SimpleImputer.fit(strategy: :median)
  |> Scholar.Impute.SimpleImputer.transform(x_train)

x_test =
  x_test
  |> Scholar.Impute.SimpleImputer.fit(strategy: :median)
  |> Scholar.Impute.SimpleImputer.transform(x_test)
```

Eventually, we can compute the correlation matrix. We will use `Scholar.Covariance` to calculate the correlation matrix.

```elixir
correlation =
  Nx.concatenate([x_train, Nx.new_axis(y_train, 1)], axis: 1)
  |> Scholar.Covariance.correlation_matrix(biased: true)
```

Maybe visual representation would be nicer. ðŸ˜…

```elixir
{corr_size, _} = Nx.shape(correlation)
correlation_list = Nx.to_flat_list(correlation)

names = [
  "Bedrooms per rooms",
  "Households",
  "Housing median age",
  "Latitude",
  "Longitude",
  "Median income",
  "Population",
  "Population per family",
  "Rooms per family",
  "Total bedrooms",
  "Total rooms",
  "Ocean proximity",
  "Median house value"
]

corr_to_plot =
  DF.new(
    x: List.flatten(List.duplicate(names, corr_size)),
    y: List.flatten(for name <- names, do: List.duplicate(name, corr_size)),
    corr_val: correlation_list
  )

Vl.new(
  title: [
    text: "Correlation Matrix for California Housing",
    offset: 20
  ],
  width: 630,
  height: 630
)
|> Vl.data_from_values(corr_to_plot)
|> Vl.mark(:rect)
|> Vl.encode_field(:x, "x", type: :nominal, title: "")
|> Vl.encode_field(:y, "y", type: :nominal, title: "")
|> Vl.encode_field(:color, "corr_val", type: :quantitative, scale: [scheme: :viridis])
|> Vl.config(axis: [grid: true, tickband: :extent])
```

We can spot that _median_house_value_ is strongly correlated with _median_income_. It's pretty straightforward, the more money you have, the more expensive house you can buy. Non-obvious is a negative correlation with _bedrooms_per_rooms_. But it also can be explained. Bedrooms are the most crucial rooms in the house. Firstly, you need to guarantee that you have a house with enough bedrooms. If this condition is satisfied, then you can focus on "additional rooms" like a chill room, cabinets and so on. So if you buy a house with more additional rooms, then you decrease the ratio.

<!-- livebook:{"break_markdown":true} -->

Now we are ready to train a model for the _median_house_value_ prediction. We will use linear regression. In the first step, we create the model by calling the `fit` function.

```elixir
model = Scholar.Linear.LinearRegression.fit(x_train, y_train)
```

Now we can predict the values for the test set and measure the error of our prediction. We will calculate root mean square error (RMSE) and mean absolute error (MAE).

```elixir
predictions = Scholar.Linear.LinearRegression.predict(model, x_test)
rmse = Scholar.Metrics.mean_square_error(y_test, predictions) |> Nx.sqrt()
mae = Scholar.Metrics.mean_absolute_error(y_test, predictions)
{rmse, mae}
```

Ok, but is it a good or poor estimation? Huh, check the mean value of the target and then compare it to the value of errors.

```elixir
Nx.mean(y)
```

For such a simple model as linear regression, it seems to be a pretty good result. But there is space to improve this result. You can, for example, add some additional features to the data set. In the future, you will be able to try more complicated models, such as random forests.
