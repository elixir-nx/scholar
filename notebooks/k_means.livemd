# K-means tutorial

```elixir
Mix.install(
  [
    {:stb_image, "~> 0.5.2"},
    {:kino, github: "livebook-dev/kino", override: true},
    {:nx, github: "elixir-nx/nx", sparse: "nx", override: true},
    {:exla, "~> 0.3"},
    {:explorer, "~> 0.2.0"},
    {:scholar, path: "/home/mateusz/Dashbit/scholar"},
    {:vega_lite, "~> 0.1.6"},
    {:jason, "~> 1.2"},
    {:kino_vega_lite, "~> 0.1.3"}
  ],
  config: [
    nx: [default_defn_options: [compiler: EXLA]]
  ]
)
```

## General Info

The main purpose of this livebook is to introduce the KMeans clustering algorithm. We will explore KMeans in three different use cases.

```elixir
import Nx.Defn
alias VegaLite, as: Vl
```

## Iris Dataset

The first example we will focus on is the **Iris Dataset**. It is one of the most renowned datasets. It consists of 150 records describing three iris species: *Iris Setosa*, *Iris Virginica*, and *Iris Versicolor*. Our task will be to predict the species of given flowers.

<!-- livebook:{"break_markdown":true} -->

Let's define module `Datasets` which will help us load data from Explorer Dataframe into the Nx tensor.

```elixir
defmodule Datasets do
  def df_to_matrix(df) do
    df
    |> Explorer.DataFrame.names()
    |> Enum.map(&(Explorer.Series.to_tensor(df[&1]) |> Nx.new_axis(-1)))
    |> Nx.concatenate(axis: 1)
  end

  def df_to_vector(df) do
    case Explorer.DataFrame.names(df) do
      [name] -> Explorer.Series.to_tensor(df[name])
      _several -> df |> df_to_matrix() |> Nx.argmax(axis: 1)
    end
  end

  defn sort_clusters(model) do
    order = Nx.argsort(model.clusters[[0..-1//1, 0]])
    labels_maping = Nx.argsort(order)

    %{
      model
      | labels: Nx.take(labels_maping, model.labels),
        clusters: Nx.take(model.clusters, order)
    }
  end
end
```

Firstly, we load the data, then we split it into Training Data (df) and Target (y) and cast it into Nx tensors.

```elixir
df_raw = Explorer.Datasets.iris()
y = Explorer.DataFrame.select(df_raw, ["species"]) |> Explorer.DataFrame.dummies(["species"])
y = Datasets.df_to_vector(y)
df = Explorer.DataFrame.select(df_raw, ["species"], :drop)
df = Datasets.df_to_matrix(df)
```

### Exploratory Data Analysis

<!-- livebook:{"break_markdown":true} -->

Important part of Data Science workflow is something called **Exploratory Data Analysis**. The EDA helps us understand the data in a better way and suggests some efficient strategies to solve problems. There is no one specific course of action which defines good EDA. It should contain tabular summaries and plots showing relations between features.

<!-- livebook:{"break_markdown":true} -->

We start our EDA by finding the mean values of each feature by specie.

```elixir
grouped_data = Explorer.DataFrame.group_by(df_raw, ["species"])

grouped_data
|> Explorer.DataFrame.summarise(
  petal_length: [:mean],
  petal_width: [:mean],
  sepal_width: [:mean],
  sepal_length: [:mean]
)
```

We see that `petal_length` and `petal_width` are the most distinguishing features. Explore them a little bit more.

```elixir
Vl.new(title: [text: "Histograms of petal_length column by specie", offset: 25])
|> Vl.data_from_values(df_raw)
|> Vl.facet(
  [field: "species"],
  Vl.new(width: 200, height: 200)
  |> Vl.mark(:bar)
  |> Vl.encode_field(:x, "petal_length", type: :quantitative, bin: true)
  |> Vl.encode_field(:y, "Frequency", aggregate: "count", scale: [domain: [0, 55]])
)
```

```elixir
Vl.new(
  width: 300,
  height: 300,
  title: [
    text: "Scatterplot of data samples pojected on plane petal_width x petal_length",
    offset: 25
  ]
)
|> Vl.data_from_values(df_raw)
|> Vl.mark(:circle)
|> Vl.encode_field(:x, "petal_length", type: :quantitative)
|> Vl.encode_field(:y, "petal_width", type: :quantitative)
|> Vl.encode_field(:color, "species")
```

```elixir
Vl.new(
  title: [
    text: "Scatterplot of data samples pojected on plane petal_width x petal_length by specie",
    offset: 25
  ]
)
|> Vl.data_from_values(df_raw)
|> Vl.facet(
  [field: "species"],
  Vl.new(width: 200, height: 200)
  |> Vl.mark(:point)
  |> Vl.encode_field(:x, "petal_length", type: :quantitative)
  |> Vl.encode_field(:y, "petal_width", type: :quantitative)
)
```

Now we have a better understanding of data. Iris species have different petal widths and petal lengths. Iris *Setosa* has the smallest petal *Versicolor* is medium size and *Virginica* has the biggest petal. We can make sure even further that our analysis is correct and plot the so-called **Elbow plot**. The Elbow plot is a plot which presents Inertia vs the number of clusters. If there is a characteristic elbow on, then we have a strong suggestion about the right number of clusters. Let's train KMeans models for a different number of clusters from range 1 to 11.

```elixir
models = for i <- 1..11, do: Scholar.Cluster.KMeans.fit(df, num_clusters: i)
inertia_of_models = for model <- models, do: Nx.to_number(model.inertia)
```

```elixir
data = [num_clusters: 1..11, inertia: inertia_of_models]

Vl.new(width: 600, height: 300, title: "Elbow Plot")
|> Vl.data_from_values(data)
|> Vl.mark(:line)
|> Vl.encode_field(:x, "num_clusters",
  title: "Number of Clusters",
  type: :ordinal,
  axis: [label_angle: 0]
)
|> Vl.encode_field(:y, "inertia", title: "Inertia", type: :quantitative)
```

As you can see, we have the elbow when the number of clusters equals three. So this value of the parameter seems to be the best.

```elixir
best_model = Enum.at(models, 2)
best_model = Datasets.sort_clusters(best_model)
accuracy = Scholar.Metrics.accuracy(best_model.labels, y)
```

Accuracy is nearly 90%, that pretty decent! Let's look at our results plotted on one of the previous plots.

```elixir
coords = best_model.clusters[[0..-1//1, 2..-1//1]] |> Nx.to_flat_list()

coords = [
  x_coords: for(i <- [0, 2, 4], do: Enum.at(coords, i)),
  y_coords: for(i <- [1, 3, 5], do: Enum.at(coords, i))
]
```

```elixir
Vl.new(
  width: 300,
  height: 300,
  title: [
    text:
      "Scatterplot of data samples pojected on plane petal_width x petal_length with calculated centroids",
    offset: 25
  ]
)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values(df_raw)
  |> Vl.mark(:circle)
  |> Vl.encode_field(:x, "petal_length", type: :quantitative)
  |> Vl.encode_field(:y, "petal_width", type: :quantitative)
  |> Vl.encode_field(:color, "species"),
  Vl.new()
  |> Vl.data_from_values(coords)
  |> Vl.mark(:circle, color: :green, size: 100)
  |> Vl.encode_field(:x, "x_coords", type: :quantitative)
  |> Vl.encode_field(:y, "y_coords", type: :quantitative)
])
```

As we expect ðŸ˜Ž

## Clustering of pixels' colors

The other interesting use case of KMeans clustering is pixel clustering. This technique replaces all pixels with similar colors (similar in terms of euclidean distance between RGB) with a centroid related to them.

<!-- livebook:{"break_markdown":true} -->

Let us start with loading the referral image.

```elixir
{:ok, img} = StbImage.read_file("../scholar/test/data/taj_mahal.jpg")
content = StbImage.to_binary(img, :jpg)
shape = img.shape
original_image = Kino.Image.new(content, :jpeg)
```

Now we will try to use only ten colors to represent the same picture.

```elixir
x = StbImage.to_nx(img) |> Nx.reshape({:auto, 3})

k_means = Scholar.Cluster.KMeans.fit(x, num_clusters: 10, num_runs: 10, max_iterations: 200)
```

```elixir
new_img = Nx.take(k_means.clusters, k_means.labels)
new_img_reshaped = new_img |> Nx.reshape(shape) |> Nx.round() |> Nx.as_type({:u, 8})
```

```elixir
binary_new_img = StbImage.from_nx(new_img_reshaped)
new_content = StbImage.to_binary(binary_new_img, :jpg)
Kino.Image.new(new_content, :jpeg)
```

Look that even though we use only ten colors, we can say without any doubt that this is the same image. Let's experiment more deeply. Now we will try 5, 10, 15, 20 and 40 colors and then compare the processed images with the original one.

```elixir
models = for i <- [5, 10, 15, 20, 40], do: Scholar.Cluster.KMeans.fit(x, num_clusters: i)
```

```elixir
new_images = for model <- models, do: Nx.take(model.clusters, model.labels)
```

```elixir
results =
  Enum.map_every(new_images, 1, fn x ->
    x
    |> Nx.reshape(shape)
    |> Nx.round()
    |> Nx.as_type({:u, 8})
    |> StbImage.from_nx()
    |> StbImage.to_binary(:jpg)
  end)
```

```elixir
kino_images = Enum.map_every(results, 1, fn x -> Kino.Image.new(x, :jpeg) end)
```

```elixir
grid_content =
  Enum.zip(
    for(i <- [5, 10, 15, 20, 40], do: Kino.Markdown.new("### Number of colors: #{i}")) ++
      [Kino.Markdown.new("### Original Image")],
    kino_images ++ [original_image]
  )

grid_content =
  Enum.reduce(grid_content, [], fn {x, y}, acc ->
    acc ++ [Kino.Layout.grid([x, y], boxed: true)]
  end)
```

```elixir
Kino.Layout.grid(grid_content, columns: 2)
```

Look that even with only five colors can recognize the Taj Mahal in the image. On the other hand with only 40 colors we keep almost all details except the sky and water surface. The reason why these two spaces do not map well is that there is a small gradient in changing colors. Pixel clustering is a great way to compress images drastically with small integration in them.

## Clustering Face Images from Olivetti Dataset

The last example is the clustering problem on Olivetti Dataset. The dataset consists of four hundred images 64 by 64 pixels. They present forty people with ten images per each one. Let's dive into this clustering problem.

<!-- livebook:{"break_markdown":true} -->

Firstly, load the data and cast it into Nx tensors.

```elixir
olivetti_data = Explorer.DataFrame.from_csv!("../scholar/test/data/olivetti_data.csv")
olivetti_target = Explorer.DataFrame.from_csv!("../scholar/test/data/olivetti_target.csv")
```

```elixir
olivetti_data = olivetti_data |> Datasets.df_to_matrix() |> Nx.transpose()
olivetti_target = olivetti_target |> Datasets.df_to_matrix() |> Nx.transpose()
```

```elixir
image_width = 64
image_height = 64

num_images = 400
```

The `ToImage` module will provide us with a function that will cast tensors into Kino.Image objects.

```elixir
defmodule ToImage do
  def tensor_to_image(x, image_height, image_width) do
    x
    |> Nx.reshape({image_height, image_width, 1})
    # 3 channels of png
    |> Nx.broadcast({image_height, image_width, 3})
    |> Nx.multiply(255)
    |> Nx.as_type({:u, 8})
    |> StbImage.from_nx()
    |> StbImage.resize(200, 200)
    |> StbImage.to_binary(:png)
    |> Kino.Image.new(:png)
  end
end
```

Here is one of the images.

```elixir
olivetti_data[[0, 0..-1//1]] |> ToImage.tensor_to_image(image_height, image_width)
```

We define the `Dist` module, which helps us implement Silhouette scoring. Function `euclidean` simply calculates the euclidean distance between a pair of tensors.

```elixir
defmodule Dist do
  deftransform assert_same_shape!(lhs, rhs) do
    lhs = Nx.shape(lhs)
    rhs = Nx.shape(rhs)

    unless lhs == rhs do
      raise ArgumentError,
            "expected input shapes to be equal," <>
              " got #{inspect(lhs)} != #{inspect(rhs)}"
    end
  end

  @spec euclidean(Nx.t(), Nx.t(), keyword()) :: Nx.t()
  defn euclidean(x, y, opts \\ []) do
    assert_same_shape!(x, y)

    opts = keyword!(opts, [:axes])

    diff = x - y

    (diff * diff)
    |> Nx.sum(axes: opts[:axes])
    |> Nx.sqrt()
  end
end
```

Now we can implement the `Silhouette` module, which will enable us to calculate the silhouette score - the indicator of how good given clustering is.

```elixir
defmodule Silhouette do
  defnp inner_dist(labels, x) do
    {num_samples, num_features} = Nx.shape(x)

    expanded_points =
      Nx.tile(x, [1, num_samples]) |> Nx.reshape({num_samples, num_samples, num_features})

    labels_vectorize = Nx.broadcast(labels, {num_samples, num_samples}) |> Nx.transpose()

    mask_zero_one =
      labels_vectorize ==
        Nx.tile(labels, [1, num_samples]) |> Nx.reshape({num_samples, num_samples})

    denominator = Nx.sum(mask_zero_one, axes: [1]) |> Nx.reshape({num_samples, 1}) |> Nx.squeeze()

    indices_to_zero = Nx.equal(denominator, 1)

    denominator = Nx.select(Nx.greater(denominator, 1), denominator - 1, 1)

    mask =
      mask_zero_one
      |> Nx.select(
        Nx.tile(Nx.iota({num_samples}), [num_samples, 1]),
        Nx.broadcast(Nx.iota({num_samples, 1}), {num_samples, num_samples})
      )

    points = Nx.take(x, mask)

    {Nx.sum(
       Dist.euclidean(
         expanded_points,
         points,
         axes: [2]
       ),
       axes: [1]
     ) / denominator, indices_to_zero}
  end

  defnp outer_dist(labels, x, opts \\ []) do
    num_clusters = opts[:num_clusters]
    {num_samples, num_features} = Nx.shape(x)
    inf = Nx.Constants.infinity()

    x_a =
      x
      |> Nx.reshape({1, num_samples, num_features})
      |> Nx.broadcast({num_samples, num_samples, num_features})

    x_b =
      x
      |> Nx.reshape({num_samples, 1, num_features})
      |> Nx.broadcast({num_samples, num_samples, num_features})

    pairwise_dist = Dist.euclidean(x_a, x_b, axes: [2])

    labels_vectorize = Nx.tile(labels, [num_clusters, 1])
    Nx.iota({num_clusters, num_samples}, axis: 0)
    mask = labels_vectorize == Nx.iota({num_clusters, num_samples}, axis: 0)
    denominator = mask |> Nx.sum(axes: [1]) |> Nx.broadcast({num_samples, num_clusters})
    results = Nx.dot(mask, pairwise_dist) |> Nx.transpose()
    results = Nx.select(Nx.transpose(mask), inf, results)
    results = results / denominator

    Nx.reduce_min(results, axes: [1])
  end

  defn silhouette_samples(labels, x, opts \\ []) do
    outer = outer_dist(labels, x, opts)
    {inner, indices_to_zero} = inner_dist(labels, x)
    result = (outer - inner) / Nx.max(outer, inner)
    Nx.select(indices_to_zero, 0, result)
  end

  defn silhouette_score(labels, x, opts \\ []) do
    Nx.mean(silhouette_samples(labels, x, opts))
  end
end
```

We will try some different numbers of clusters and then compare which clustering was the best with `silhouette_score`. The following cell might calculate a while.

```elixir
models =
  for num_clusters <- [40, 80, 120, 160],
      do: Scholar.Cluster.KMeans.fit(olivetti_data, num_clusters: num_clusters)
```

```elixir
silhouette_scores =
  for {model, num_clusters} <- Enum.zip(models, [40, 80, 120, 160]),
      do:
        Nx.to_number(
          Silhouette.silhouette_score(model.labels, olivetti_data, num_clusters: num_clusters)
        )
```

```elixir
data = [num_clusters: [40, 80, 120, 160], silhouette_scores: silhouette_scores]

Vl.new(width: 600, height: 300, title: "Silhouette score vs Number of Clusters")
|> Vl.data_from_values(data)
|> Vl.layers([
  Vl.new()
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "num_clusters",
    title: "Number of Clusters",
    type: :ordinal,
    axis: [label_angle: 0]
  )
  |> Vl.encode_field(:y, "silhouette_scores",
    title: "Silhouette score",
    type: :quantitative,
    scale: [domain: [0.12, 0.165]]
  ),
  Vl.new()
  |> Vl.mark(:circle, size: 50, color: :dark_blue)
  |> Vl.encode_field(:x, "num_clusters")
  |> Vl.encode_field(:y, "silhouette_scores", type: :quantitative)
])
```

As we can see, the model with num_clusters equal to 120 has the best performance. It might be a little surprise since we have only forty classes originally. But with forty clusters, lots of images are misclassified. Now we will visualize the best clusterization.

```elixir
num_clusters = 120
best_model = Enum.at(models, 2)
```

```elixir
labels = Nx.to_flat_list(best_model.labels)
```

```elixir
labels = Enum.zip([labels, 0..(num_images - 1), Nx.to_flat_list(olivetti_target)])
```

```elixir
clustering_map = Map.new(0..(num_clusters - 1), &{&1, []})

mapping =
  Enum.reduce(labels, clustering_map, fn {x, idx, ref}, acc ->
    Map.update(acc, x, nil, fn x ->
      x ++
        [
          {ToImage.tensor_to_image(olivetti_data[[idx, 0..-1//1]], image_height, image_width),
           ref}
        ]
    end)
  end)
```

```elixir
preprocessing =
  Enum.reduce(0..(num_clusters - 1), [], fn i, acc ->
    acc ++
      [
        Kino.Layout.grid(
          [Kino.Markdown.new("## Cluster no. #{i}")] ++
            [
              Kino.Layout.grid(
                Enum.reduce(mapping[i], [], fn {image, idx}, acc1 ->
                  acc1 ++
                    [
                      Kino.Layout.grid([
                        Kino.Markdown.new("Reference Cluster no. #{idx}"),
                        image
                      ])
                    ]
                end),
                columns: 3
              )
            ],
          boxed: true
        )
      ]
  end)
```

```elixir
Kino.Layout.grid(preprocessing)
```

There are clusters which seem to be ok, but unfortunately, a few of them are wrong or contain only one image. KMeans is a powerful tool, but in this task, CNNs might perform much better.
